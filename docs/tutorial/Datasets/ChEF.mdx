---
title: 'ChEF'
sidebar_position: 3
---

# ChEF

## LAMM

ChEF is compatible with original LAMM benchmark. Download LAMM 2D Benchmark datasets. More details are in [LAMM](/tutorial/Datasets/LAMM). 

**Data Structure**

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        └── LAMM
            └── 2D_Benchmark
                ├── cifar10_images
                ├── flickr30k_images
                ├── fsc147_images
                ├── meta_file
                ├── sqaimage_images
                └── voc2012_images
```

## Omnibenchmark

Download [Omnibenchmark](https://entuedu-my.sharepoint.com/:f:/g/personal/yuanhan002_e_ntu_edu_sg/El2wmbzutJBOlu8Tz9HyDJABMmFtsG_8mq7uGh4Q7F1QSQ?e=NyroDS) for fine-grained classification dataset and [Bamboo Label System](https://github.com/ZhangYuanhan-AI/Bamboo) for hierarchical catergory labels. 

**Data Structure**

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        ├── Omnibenchmark_raw
        └── Bamboo
            └── sensexo_visual_add_academic_add_state_V4.visual.json
```

We sampled and labeled Omnibenchmark meticulously by using a hierarchical chain of categories, facilitated by the Bamboo label system. 

```shell
python ChEF/data_process/Omnibenchmark.py
```

You can also directly download the labeled Omnibenchmark dataset from [OpenXLab](https://openxlab.org.cn/datasets/LAMM/ChEF).

Finally, the dataset should have this structure:

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        ├── ChEF
        |   └── Omnibenchmark_Bamboo
        |       ├── meta_file
        |       └── omnibenchmark_images
        └── Bamboo
            └── sensexo_visual_add_academic_add_state_V4.visual.json
```

## MMBench, MME and SEEDBench
Refer to [MMBench](https://github.com/open-compass/MMBench), [MME](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models) and [SEEDBench](https://github.com/AILab-CVC/SEED-Bench) for dataset and more details.

**Data Structure**

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        ├── MMBench
        |   ├── mmbench_dev_20230712.tsv
        |   └── mmbench_test_20230712.tsv
        ├── MME_Benchmark_release_version
        └── SEED-Bench
```

## POPE
POPE is a special labeled COCO dataset for hallucination evaluation based on the validation set of COCO 2014. Download [COCO](https://cocodataset.org/#download)  and [POPE](https://github.com/RUCAIBox/POPE).

**Data Structure**

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        └── coco_pope
            ├── val2014
            ├── coco_pope_adversarial.json
            ├── coco_pope_popular.json
            └── coco_pope_random.json
```

## MMBench_C and ScienceQA_C
MMBench_C and ScienceQA_C are datasets with image and text corruptions fot robustness evaluation. You can also directly download the MMBench_C and ScienceQA_C dataset from [OpenXLab](https://openxlab.org.cn/datasets/LAMM/ChEF).

Data Structure

```text
ChEF
├── configs
└── data
    ├── checkpoints
    └── datasets
        └── ChEF
            ├── MMBench_C
            |   ├── images
            |   ├── Image_Corruptions_info.json
            |   ├── Text_Corruptions_info.json
            |   └── MMBench_C.json
            └── ScienceQA_C
                ├── sqaimage_images
                ├── Image_Corruptions_info.json
                ├── Text_Corruptions_info.json
                └── VQA_ScienceQA_C.json
```

