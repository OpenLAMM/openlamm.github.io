---
title: 'Model Zoo'
sidebar_position: 3
---

# Model Zoo

Download pretrained weights of visual encoder and LLM and several pretrained models for evalution.

#### Pretrained Model Weights for Visual Encoder and LLM

|  Model Name  | Link  |  
|  ----------  | ----  |   
|  Vicuna      | [Link](https://github.com/lm-sys/FastChat/tree/main#vicuna-weights)  | 
| clip-vit-large-patch14-336  | [Link](https://huggingface.co/openai/clip-vit-large-patch14-336) | 
| epcl_vit-L_256tokens        | [Link](https://huggingface.co/openlamm/epcl_vit-L_256tokens)     |                   


#### LAMM Model Zoo

| # Training Samples  | Vision Encoder | LLM | Training Data | Lora Rank | Link |
| -------------------------- | :--------: | :--------: | -------- | :----: | :---------------: |
| 98K  | CLIP-ViT-L | Vicuna_v0_7B            | LAMM-2D daily dialogue & desctiption | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm_7b_lora32_98k) |
| 186K  | CLIP-ViT-L | Vicuna_v0_7B            | LAMM-2D Instruction Data | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm_7b_lora32_186k) |
| 186K | CLIP-ViT-L |  LLaMA2_chat_7B           | LAMM-2D Instruction Data | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm186k_llama2chat7b_lora32) |
| 98K | CLIP-ViT-L | Vicuna_v0_13B           | LAMM-2D daily dialogue & desctiption | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm_13b_lora32_98k) |
| 186K | CLIP-ViT-L |  Vicuna_v0_13B           | LAMM-2D Instruction Data | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm_13b_lora_186k) |
| 10K | EPCL-ViT-L |  Vicuna_v0_13B           | LAMM-3D Instruction Data | 32 | [Checkpoints](https://huggingface.co/openlamm/lamm3d_13b_lora32_10k) |


#### Octavius Model Zoo

|  Model Name  | Link  |  
|  ----------  | ----  |   
|  Octavius_2d-3d_e6_bs64      | [Checkpoints](https://huggingface.co/openlamm/Octavius_2d-3d_e6_bs64)  | 
  
#### ChEF Model Zoo

The evaluated MLLMs in ChEF:

| LLM              | Vision Encoder | Language Model | Link                                                              |
| :--------       | :--------     | :------------ | :--------------------------------------------------------------- |
| [InstructBLIP](https://github.com/salesforce/LAVIS)     | EVA-G          |    Vicuna 7B   | [instruct_blip_vicuna7b_trimmed](https://storage.googleapis.com/sfr-vision-language-research/LAVIS/models/InstructBLIP/instruct_blip_vicuna7b_trimmed.pth) |
| [Kosmos2](https://github.com/microsoft/unilm/tree/master/kosmos-2)          | CLIP ViT-L/14  |  Decoder 1.3B  | [kosmos-2.pt](https://conversationhub.blob.core.windows.net/beit-share-public/kosmos-2/kosmos-2.pt?sv=2021-10-04&st=2023-06-08T11%3A16%3A02Z&se=2033-06-09T11%3A16%3A00Z&sr=c&sp=r&sig=N4pfCVmSeq4L4tS8QbrFVsX6f6q844eft8xSuXdxU48%3D) |
| [LAMM](https://github.com/OpenLAMM/LAMM)             | CLIP ViT-L/14  |  Vicuna 13B    | [lamm_13b_lora32_186k](https://huggingface.co/openlamm/lamm_13b_lora32_186k) |
| [LLaMA-Adapter-v2](https://github.com/ml-lab/LLaMA-Adapter-2) | CLIP ViT-L/14  |    LLaMA 7B    | [LORA-BIAS-7B](https://github.com/ml-lab/LLaMA-Adapter-2) |
| [LLaVA](https://github.com/haotian-liu/LLaVA)            | CLIP ViT-L/14  |    MPT 7B      | [LLaVA-Lightning-MPT-7B](https://huggingface.co/liuhaotian/LLaVA-Lightning-MPT-7B-preview) |
| [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)        | EVA-G          |   Vicuna 7B    | [MiniGPT-4](https://huggingface.co/Vision-CAIR/MiniGPT-4) |
| [mPLUG-Owl](https://github.com/X-PLUG/mPLUG-Owl)        | CLIP ViT-L/14  |    LLaMA 7B    | [mplug-owl-llama-7b](https://huggingface.co/MAGAer13/mplug-owl-llama-7b) |
| [Otter](https://github.com/Luodian/Otter)            | CLIP ViT-L/14  |    LLaMA 7B    | [OTTER-9B-LA-InContext](https://huggingface.co/luodian/OTTER-Image-LLaMA7B-LA-InContext) |
| [Shikra](https://github.com/shikras/shikra)           | CLIP ViT-L/14  |    LLaMA 7B    | [shikra-7b](https://huggingface.co/shikras/shikra-7b-delta-v1) |


#### Data Structure



### Requirements

Please refer to the respective repositories of each model for their requirements.

### Custom Model Configuration

Define the model configs in `src/config/ChEF/models`, including `model_name`, `model_path` and other neccessary configs. For certain models, different configurations are required when testing on different recipes. For example, the default config for `KOSMOS-2`(src/config/ChEF/models/kosmos2.yaml):

```yaml
model_name: Kosmos2
model_path: ../model_zoo/kosmos/kosmos-2.pt
if_grounding: False # set True for detection and grounding evaluation
``` 
The config for KOSMOS-2 on detection tasks evaluation:
```yaml
model_name: Kosmos2
model_path: ../model_zoo/kosmos/kosmos-2.pt
if_grounding: True
``` 
