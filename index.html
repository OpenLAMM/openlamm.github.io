<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LAMM</title>
  <link rel="icon" type="image/x-icon" href="static/images/lamm.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/style.css" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/lamm.png" align="left" width="150px" hspace="15"/> 
              LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=ngPR1dIAAAAJ&hl=zh-CN" target="_blank">Zhenfei Yin</a><sup>1*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/wangjiongw" target="_blank">Jiong Wang</a><sup>1, 3*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=S25Lr3oAAAAJ&hl=zh-CN" target="_blank">Jianjian Cao</a><sup>1, 4*</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EDLcoVkAAAAJ&hl=zh-CN" target="_blank">Zhelun Shi</a><sup>1, 2*</sup>,</span>
              <span class="author-block">
                <a href="https://github.com/lnbxldn" target="_blank">Dingning Liu</a><sup>1, 5</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=BizedOAAAAAJ" target="_blank">Mukai Li</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://lucassheng.github.io/" target="_blank">Lu Sheng</a><sup></sup><sup>2</sup>,</span>  
              <span class="author-block">
                <a href="http://leibai.site/" target="_blank">Lei Bai</a><sup></sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://xiaoshuihuang.github.io/" target="_blank">Xiaoshui Huang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=Sqou_P0AAAAJ&hl=en" target="_blank">Zhiyong Wang</a><sup>6</sup>,</span>
              <span class="author-block">
                <a href="https://wlouyang.github.io/" target="_blank">Wanli Ouyang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://amandajshao.github.io/" target="_blank">Jing Shao</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Shanghai Artificial Intelligence Lab, <sup>2</sup>Beihang University, <sup>3</sup>The Chinese University of Hong Kong (Shenzhen), 
                <sup>4</sup>Fudan University, <sup>5</sup>Dalian University of Technology, <sup>6</sup>The University of Sydney<br>
                NeurIPS Dataset & Benchmark
                Track 2023</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/OpenLAMM/LAMM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/OpenLAMM/LAMM#data-download" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!-- Teaser video-->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
          <!-- Your video here -->
        <img src="static/images/lamm.png" alt="MY ALT TEXT" />
        <!-- </video> -->
        <!-- <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2> -->
      </div>
    </div>
  </section>
  <!-- End teaser video -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Large language models have become a potential pathway toward achieving artificial general intelligence.
              Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual
              modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and
              LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to
              facilitate the extension of MLLMs to additional modalities.
              Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost
              all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our
              dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets
              and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other
              domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework
              optimized for modalities' extension. We also provide baseline models, comprehensive experimental
              observations, and analysis to accelerate future research.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->



  <!-- Image carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/conversation.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Conversations.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/detail.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Detailed Description.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/subset.png" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Additional Data.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End image carousel -->


  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">LAMM</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/Benckmark.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              LAMM Benchmark
            </h2>
          </div>
          <div class="item">
            <img src="static/images/DATASET.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              LAMM Dataset
            </h2>
          </div>
          <div class="item">
            <img src="static/images/architecture.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Framework
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Dataset Examples</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/conversation.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Daily Dialogue
            </h2>
          </div>
          <div class="item">
            <img src="static/images/detail.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Detailed Description.
            </h2>
          </div>
          <div class="item">
            <img src="static/images/subset.svg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Towards Fatual knowledge & CV Tasks
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->


<!-- Leaderboard -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Leaderboard</h2>
        <!--<div class="content has-text-justified">
        
          <p>
            Large language models have become a potential pathway toward achieving artificial general intelligence.
            Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual
            modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and
            LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to
            facilitate the extension of MLLMs to additional modalities.
            Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost
            all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our
            dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets
            and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other
            domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework
            optimized for modalities' extension. We also provide baseline models, comprehensive experimental
            observations, and analysis to accelerate future research.
          </p>-->
          <div id="container">   

    
            <table class="lamm">
            <caption><b>Comparison of Multimodal Large Language Models on 2D computer vision tasks</b></caption>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Dataset</th>
                        <th>Metric</th>
                        <th>SOTA</th>
                        <th>LLaVA</th>
                        <th>MiniGPT4</th>
                        <th>mPLUG-owl</th>
                        <th>LAMM</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Classification</td>
                        <td>CIFAR10</td>
                        <td>Acc ↑</td>
                        <td>99.5</td>
                        <td><b><u>60.83</u></b></td>
                        <td>46.22</td>
                        <td>42.5</td>
                        <td>34.5</td>
                    </tr>
                    <tr>
                        <td>Detection</td>
                        <td>VOC2012</td>
                        <td>mAP ↑</td>
                        <td>97.2</td>
                        <td>1.42</td>
                        <td>0.92</td>
                        <td>0.158</td>
                        <td><b><u>4.82</u></b></td>
                    </tr>
                    <tr>
                        <td>VQA</td>
                        <td>SQAimage<br>AI2D</td>
                        <td>Acc ↑</td>
                        <td>92.53<br>N/A</td>
                        <td>40.5<br>18.13</td>
                        <td>43.43<br>Failed</td>
                        <td>36.39<br>19.31</td>
                        <td><b><u>47.15<br>19.5</u></b></td>
                    </tr><tr>
                        <td>Image Caption</td>
                        <td>flickr30k</td>
                        <td>BLEU4 ↑</td>
                        <td>30.1</td>
                        <td><b><u>6.65</u></b></td>
                        <td>5.1</td>
                        <td>2.74</td>
                        <td>0.70</td>
                    </tr>
                    <tr>
                        <td>F-g clasification</td>
                        <td>UCMerced</td>
                        <td>Acc ↑</td>
                        <td>100</td>
                        <td><b>47</b></td>
                        <td>33.6</td>
                        <td>32.5</td>
                        <td>13</td>
                    </tr>
                    <tr>
                        <td>Counting</td>
                        <td>FSC147</td>
                        <td>MAE ↓</td>
                        <td>10.79</td>
                        <td>56.2</td>
                        <td>Failed</td>
                        <td>60.67</td>
                        <td><b><u>53.97</u></b></td>
                    </tr>
                    <tr>
                        <td>OCR</td>
                        <td>SVT</td>
                        <td>Word Acc ↑</td>
                        <td>97.9</td>
                        <td><b><u>37.78</u></b></td>
                        <td>16.97</td>
                        <td>30.39</td>
                        <td>4.2</td>
                    </tr>
                    <tr>
                        <td>Facial Classification</td>
                        <td>CelebA(Smile)<br>CelebA(Hair)</td>
                        <td>Acc ↑</td>
                        <td>N/A<br>N/A</td>
                        <td>Failed<br><b><u>46.42</u></b></td>
                        <td><b><u>66.36</u></b><br>43.47</td>
                        <td>Failed<br>40.93</td>
                        <td>51.3<br>30.48</td>
                    </tr>
                    <tr>
                        <td>Keypoints Detection</td>
                        <td>LSP</td>
                        <td>PCK ↑</td>
                        <td>99.5</td>
                        <td>Failed</td>
                        <td>Failed</td>
                        <td>Failed</td>
                        <td>Failed</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id="container">   

            
            <table class="lamm">
            <caption><b>Results of LAMM model on selected 2D vision tasks</b></caption>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Dataset</th>
                        <th>LAMM(Zero-Shot)</th>
                        <th>LAMM(Finetune)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Classification (Acc)</td>
                        <td>CIFAR10</td>
                        <td>34.5</td>
                        <td>91.2</td>
                    </tr>
                    <tr>
                        <td>Object Detection (Acc)</td>
                        <td>VOC2012</td>
                        <td>4.82</td>
                        <td>13.48</td>
                    </tr>
                    <tr>
                        <td>VQA (mAP@0.5)</td>
                        <td>SQAimage</td>
                        <td>47.15</td>
                        <td>74.27</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div id="container">   

            
            <table class="lamm">
            <caption><b>Results of 3D tasks by LAMM</b></caption>
                <thead>
                    <tr>
                        <th>Task</th>
                        <th>Dataset</th>
                        <th>SOTA</th>
                        <th>LAMM(Zero-Shot)</th>
                        <th>LAMM(Finetune)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>3D Object Detection <b>(mAP@0.5)</b></td>
                        <td>ScanNet</td>
                        <td>63.2</td>
                        <td>9.3</td>
                        <td>11.89</td>
                    </tr>
                    <tr>
                        <td>Visual Grounding <b>(mAP@0.5)</b></td>
                        <td>ScanRefer</td>
                        <td>54.59</td>
                        <td>Failed</td>
                        <td>3.38</td>
                    </tr>
                    <tr>
                        <td>3D VQA <b>(Acc of multiple choice prolblem)</b></td>
                        <td>ScanQA</td>
                        <td>N/A</td>
                        <td>26.54</td>
                        <td>99.89</td>
                    </tr>
                </tbody>
            </table>
        </div>       
        <div id="container">   

            
            <table class="lamm">
            <caption><b>Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs</b></caption>
                <thead>
                    <tr>
                        <th></th>
                        <th>LLaVA</th>
                        <th>MiniGPT4</th>
                        <th>mPLUG-owl</th>
                        <th>LAMM</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Binary-Loc Metric</td>
                        <td>14.73</td>
                        <td>13.12</td>
                        <td>4.42</td>
                        <td><b><u>31.2</u></b></td>
                    </tr>
                    <tr>
                        <td>GPT Metric</td>
                        <td>11</td>
                        <td>-</td>
                        <td>-</td>
                        <td><b><u>89</u></b></td>
                    </tr>
                    
                </tbody>
            </table>
        </div> 

        
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">

            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End video carousel -->




  <!-- Paper poster -->
  <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/LAMM.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section>
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a>.
              <!-- You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative -->
              <!-- Commons Attribution-ShareAlike 4.0 International License</a>. -->
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>