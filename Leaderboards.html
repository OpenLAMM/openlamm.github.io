<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">Leaderboards | LAMM</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://openlamm.github.io/logo/LAMM-logo.png"><meta data-rh="true" name="twitter:image" content="https://openlamm.github.io/logo/LAMM-logo.png"><meta data-rh="true" property="og:url" content="https://openlamm.github.io/Leaderboards"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Leaderboards | LAMM"><meta data-rh="true" name="description" content="Visual performance of MLLMs on different Scenarios"><meta data-rh="true" property="og:description" content="Visual performance of MLLMs on different Scenarios"><link data-rh="true" rel="icon" href="/logo/LAMM-logo.png"><link data-rh="true" rel="canonical" href="https://openlamm.github.io/Leaderboards"><link data-rh="true" rel="alternate" href="https://openlamm.github.io/Leaderboards" hreflang="en"><link data-rh="true" rel="alternate" href="https://openlamm.github.io/Leaderboards" hreflang="x-default"><link data-rh="true" rel="stylesheet" href="/assets/css/docsly.min.css"><link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-5FDFFSS",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script>





<script src="https://cdn.statuspage.io/se-v2.js"></script><link rel="stylesheet" href="/assets/css/styles.959e90f6.css">
<link rel="preload" href="/assets/js/runtime~main.7f68de47.js" as="script">
<link rel="preload" href="/assets/js/main.d6ae70d1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5FDFFSS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/logo/LAMM-logo-light.png" alt="LAMM" class="themedImage_ToTc themedImage--light_HNdA" height="60px"><img src="/logo/LAMM-logo-dark.png" alt="LAMM" class="themedImage_ToTc themedImage--dark_i4oU" height="60px"></div></a><a class="navbar__item navbar__link" href="/tutorial">Tutorial</a><a class="navbar__item navbar__link" href="/datasets">Datasets</a><a class="navbar__item navbar__link" href="/model_system_card">Models</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/Leaderboards">Leaderboards</a><a class="navbar__item navbar__link" href="/Team">Team</a><a href="https://github.com/OpenGVLab/LAMM" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><h1>Leaderboards</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="visual-performance-of-mllms-on-different-scenarios">Visual performance of MLLMs on different Scenarios<a href="#visual-performance-of-mllms-on-different-scenarios" class="hash-link" aria-label="Direct link to Visual performance of MLLMs on different Scenarios" title="Direct link to Visual performance of MLLMs on different Scenarios">​</a></h2><p>For each <em>Scenario</em>, we conduct various experiments with diverse <em>Recipes</em>, from which, the <em>Recipe</em> behaving most reliably (<em>i.e.</em> stable to <em>Instruction</em> variations) is selected as the default setting to evaluate the visual performance of all MLLMs.</p><table><thead><tr><th><strong>Scenario</strong></th><th><strong>CIFAR</strong></th><th><strong>Flickr</strong></th><th><strong>VOC</strong></th><th><strong>Omni</strong></th><th><strong>FSC</strong></th><th><strong>SQA</strong></th><th><strong>MM</strong></th><th><strong>SEED</strong></th><th><strong>MME</strong></th></tr></thead><tbody><tr><td><strong>LLaVA</strong></td><td><strong>89.40</strong></td><td>80.80</td><td>26.01</td><td>26.62</td><td>24.11</td><td>46.55</td><td>43.13</td><td>46.45</td><td>50.17</td></tr><tr><td><strong>LAMM</strong></td><td>80.70</td><td>72.50</td><td>29.58</td><td>22.54</td><td>19.33</td><td>52.75</td><td>44.47</td><td>47.03</td><td>55.82</td></tr><tr><td><strong>MiniGPT-4</strong></td><td>80.80</td><td>71.50</td><td>26.51</td><td>30.60</td><td>22.52</td><td>47.0</td><td>54.34</td><td>46.48</td><td>57.12</td></tr><tr><td><strong>mPLUG-owl</strong></td><td>79.67</td><td>79.20</td><td>28.50</td><td>30.70</td><td>20.92</td><td>48.44</td><td>49.57</td><td>42.81</td><td>71.59</td></tr><tr><td><strong>Otter</strong></td><td>81.34</td><td>71.30</td><td>27.15</td><td>26.41</td><td>20.00</td><td>50.22</td><td>53.91</td><td>36.40</td><td>63.78</td></tr><tr><td><strong>LlaMA-Adapter2</strong></td><td>70.17</td><td>79.50</td><td>31.60</td><td><strong>32.00</strong></td><td>21.26</td><td>54.34</td><td>57.06</td><td>35.41</td><td>69.90</td></tr><tr><td><strong>InstructBLIP</strong></td><td>84.27</td><td>79.40</td><td>27.65</td><td>30.75</td><td><strong>25.04</strong></td><td><strong>55.18</strong></td><td><strong>65.73</strong></td><td><strong>50.81</strong></td><td><strong>72.0</strong></td></tr><tr><td><strong>Shikra</strong></td><td>68.71</td><td><strong>94.70</strong></td><td><strong>55.23</strong></td><td>22.89</td><td>22.43</td><td>45.21</td><td>63.26</td><td>49.79</td><td>70.28</td></tr><tr><td><strong>Kosmos-2</strong></td><td>88.87</td><td>85.70</td><td>54.55</td><td>21.34</td><td>21.93</td><td>34.60</td><td>32.82</td><td>46.38</td><td>52.95</td></tr><tr><td><strong>Random Choice</strong></td><td>10.0</td><td>25.00</td><td>25.00</td><td>10.94</td><td>20.00</td><td>35.80</td><td>27.57</td><td>24.27</td><td>50.00</td></tr></tbody></table><p>*CIFAR denotes CIFAR-10, Flickr denotes Flickr30k, VOC denotes VOC2012, Omni denotes Omnibenchmark, FSC denotes FSC147, SQA denotes ScienceQA, MM denotes MMbench, and SEED denotes Seedbench.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="results-of-desiderata">Results of Desiderata<a href="#results-of-desiderata" class="hash-link" aria-label="Direct link to Results of Desiderata" title="Direct link to Results of Desiderata">​</a></h2><p>We employ specialized <em>Recipes</em> to assess the six dimensions of desiderata. All the six dimensions of desiderata except language performance and hallucination are evaluated on MMBench and ScienceQA. Language performance is evaluated on 250 samples random retrieved from ScienceQA and MMBench. Following POPE, hallucination is specifically assessed on the MSCOCO dataset.</p><table><thead><tr><th><strong>Desiderata</strong></th><th><strong>Calibration</strong></th><th><strong>ICL</strong></th><th><strong>Ins. Follow.</strong></th><th><strong>Lang. Perf.</strong></th><th><strong>Hallucination</strong></th><th><strong>Robustness</strong></th></tr></thead><tbody><tr><td><strong>LLaVA</strong></td><td>90.1</td><td>15.15</td><td>44.23</td><td>84.82</td><td>50.51</td><td>63.36</td></tr><tr><td><strong>LAMM</strong></td><td>76.36</td><td>40.17</td><td>40.01</td><td>79.08</td><td>57.42</td><td>57.98</td></tr><tr><td><strong>MiniGPT-4</strong></td><td>84.73</td><td>36.85</td><td>43.73</td><td>76.00</td><td>71.30</td><td>60.40</td></tr><tr><td><strong>mPLUG-owl</strong></td><td>84.15</td><td>33.45</td><td>36.73</td><td>88.44</td><td>52.26</td><td>51.05</td></tr><tr><td><strong>Otter</strong></td><td>82.80</td><td><strong>48.31</strong></td><td>38.40</td><td>74.05</td><td>54.54</td><td>57.16</td></tr><tr><td><strong>LlaMA-Adapter2</strong></td><td>89.61</td><td>36.52</td><td>38.76</td><td><strong>90.85</strong></td><td>63.83</td><td>65.37</td></tr><tr><td><strong>InstructBLIP</strong></td><td><strong>91.25</strong></td><td>46.14</td><td><strong>44.59</strong></td><td>80.01</td><td><strong>84.81</strong></td><td><strong>72.85</strong></td></tr><tr><td><strong>Shikra</strong></td><td>88.35</td><td>30.21</td><td>36.21</td><td>66.67</td><td>83.78</td><td>47.91</td></tr><tr><td><strong>Kosmos-2</strong></td><td>89.19</td><td>10.72</td><td>17.62</td><td>45.86</td><td>50.50</td><td>22.69</td></tr></tbody></table><p>*ICL denotes In-context learning, Ins. Follow. denotes Instruction Following, and Lang. Perf. denotes Language Performance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="evaluation-on-gpt-4v-and-bard">Evaluation on GPT-4V and Bard<a href="#evaluation-on-gpt-4v-and-bard" class="hash-link" aria-label="Direct link to Evaluation on GPT-4V and Bard" title="Direct link to Evaluation on GPT-4V and Bard">​</a></h2><p>We evaluate GPT-4V(ision) and Bard on MMBench and ScienceQA scenarios, as well as the desiderata including in-context learning, instruction following, hallucination, and robustness. We extract 30 data samples from ScienceQA and MMBench respectively for both scenario evaluations and each of the desideratum evaluation. We compare these two api-only models with three open-source MLLMs (LLaVA, Otter, and MiniGPT4) on the same data samples.</p><table><thead><tr><th><strong>MLLM</strong></th><th><strong>ScienceQA</strong></th><th><strong>MMBench</strong></th><th><strong>ICL</strong></th><th><strong>Ins. Follow.</strong></th><th><strong>Robustness</strong></th><th><strong>Hallucination</strong></th></tr></thead><tbody><tr><td><strong>GPT-4V</strong></td><td><strong>96.67</strong></td><td><strong>93.80</strong></td><td>43.98</td><td><strong>97.69</strong></td><td><strong>82.16</strong></td><td><strong>96.00</strong></td></tr><tr><td><strong>Bard</strong></td><td>90.00</td><td>71.43</td><td>39.61</td><td>71.41</td><td>71.05</td><td>88.88</td></tr><tr><td><strong>LLaVA</strong></td><td>50.00</td><td>43.33</td><td><strong>47.99</strong></td><td>36.67</td><td>34.18</td><td>36.67</td></tr><tr><td><strong>Otter</strong></td><td>63.33</td><td>50.00</td><td>47.91</td><td>44.44</td><td>37.35</td><td>80.00</td></tr><tr><td><strong>mPLUG-Owl</strong></td><td>53.33</td><td>46.67</td><td>42.14</td><td>41.67</td><td>63.46</td><td>36.67</td></tr></tbody></table><p>*ICL denotes In-context learning, and Ins. Follow. denotes Instruction Following.</p></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#visual-performance-of-mllms-on-different-scenarios" class="table-of-contents__link toc-highlight">Visual performance of MLLMs on different Scenarios</a></li><li><a href="#results-of-desiderata" class="table-of-contents__link toc-highlight">Results of Desiderata</a></li><li><a href="#evaluation-on-gpt-4v-and-bard" class="table-of-contents__link toc-highlight">Evaluation on GPT-4V and Bard</a></li></ul></div></div></div></main></div><div class="mx-auto flex items-center w-full max-w-[1080px] flex-col px-6 py-12"><div class="flex flex-col gap-6 lg:flex-row lg:items-center lg:justify-between lg:gap-0"><div class="flex items-center gap-4"><div class="flex flex-wrap gap-2 text-sm text-gray-500"><span class="text-inherit">© <!-- -->2024<!-- --> LAMM. Built with Dyte.</span></div></div></div></div></div>
<script src="/assets/js/runtime~main.7f68de47.js"></script>
<script src="/assets/js/main.d6ae70d1.js"></script>
</body>
</html>