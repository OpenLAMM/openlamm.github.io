"use strict";(self.webpackChunkdyte_docs=self.webpackChunkdyte_docs||[]).push([[8030],{2410:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>s,default:()=>m,frontMatter:()=>l,metadata:()=>c,toc:()=>d});t(7294);var i=t(3905);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){return n=null!=n?n:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):function(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,i)}return t}(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})),e}function o(e,n){if(null==e)return{};var t,i,a=function(e,n){if(null==e)return{};var t,i,a={},r=Object.keys(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)t=r[i],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}const l={title:"Installation",sidebar_position:3},s=void 0,c={unversionedId:"installation/installation",id:"installation/installation",title:"Installation",description:"Training",source:"@site/docs/tutorial/installation/installation.mdx",sourceDirName:"installation",slug:"/installation/",permalink:"/tutorial/installation/",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Installation",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Benchmarking",permalink:"/tutorial/Datasets/benchmark"},next:{title:"Training",permalink:"/tutorial/Training/"}},p={},d=[{value:"Training",id:"training",level:2},{value:"Python &amp; Pytorch Environment",id:"python--pytorch-environment",level:4},{value:"Install Required Dependencies",id:"install-required-dependencies",level:4},{value:"Optional Dependencies",id:"optional-dependencies",level:4},{value:"Benchmarking",id:"benchmarking",level:2},{value:"Optional Dependencies",id:"optional-dependencies-1",level:4}],u={toc:d};function m(e){var{components:n}=e,t=o(e,["components"]);return(0,i.kt)("wrapper",r(function(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{},i=Object.keys(t);"function"==typeof Object.getOwnPropertySymbols&&(i=i.concat(Object.getOwnPropertySymbols(t).filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable})))),i.forEach((function(n){a(e,n,t[n])}))}return e}({},u,t),{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h2",{id:"training"},"Training"),(0,i.kt)("p",null,"Pre-requist Packages: ",(0,i.kt)("inlineCode",{parentName:"p"},"gcc <= 7.5.0; nvcc >= 11.1")),(0,i.kt)("h4",{id:"python--pytorch-environment"},"Python & Pytorch Environment"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"```bash\nconda create -n lamm python=3.10 -y\nconda activate lamm\n# Choose different version of torch according to your \nconda install pytorch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1 cudatoolkit=11.3 -c pytorch\n```\n")),(0,i.kt)("h4",{id:"install-required-dependencies"},"Install Required Dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"```bash\nconda install timm==0.6.7 deepspeed==0.9.3 transformers==4.31.0 -c conda-forge\npip install peft==0.3.0 --no-dependencies\npip install -r requirements/default.txt\n\n```\nInstall Faiss \n```bash\n# if cuda is available\nconda install -c conda-forge faiss-gpu\n# otherwise\nconda install -c conda-forge faiss-cpu\n```\nDownload required NLTK data\n\n```python\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n```\n")),(0,i.kt)("h4",{id:"optional-dependencies"},"Optional Dependencies"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre"},"* LAMM-3D Environments\n\n    ```bash\n    cd src/model/EPCL/third_party/pointnet2/\n    python setup.py install\n    cd ../../utils/\n    pip install cython\n    python cython_compile.py build_ext --inplace\n    ```\n\n* Reducing Memory in Training\n\n    - flash attention (v2)   \n        \n        Install flash attention (v2) if you are tight in GPU memory. Please refer to [flash attention's installation](https://github.com/Dao-AILab/flash-attention/tree/main#installation-and-features)\n        \n        > FlashAttention-2 currently supports Ampere, Ada, or Hopper GPUs (e.g., A100, RTX 3090, RTX 4090, H100).\n\n    - xformers   \n\n        Install xformers if you are tight in GPU memory and cannot use flash attention (e.g., using Nvidia v100). Please refer to [xformers's installation](https://github.com/facebookresearch/xformers#installing-xformers)\n")),(0,i.kt)("h2",{id:"benchmarking"},"Benchmarking"),(0,i.kt)("p",null,"We use ChEF for benchmarking."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-shell"},"conda create -n ChEF python=3.10\nconda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\npip install -r requirements/default.txt\npip install -r requirements/ChEF.txt\n")),(0,i.kt)("h4",{id:"optional-dependencies-1"},"Optional Dependencies"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"Efficient Inference"),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},"lightllm   "),(0,i.kt)("p",{parentName:"li"},"  Install lightllm to speed up inference and decrease the GPU memery usage to enable large batchsize."),(0,i.kt)("pre",{parentName:"li"},(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"git clone -b multimodal  https://github.com/ModelTC/lightllm.git\ncd lightllm\npython setup.py install\n")))))))}m.isMDXComponent=!0}}]);