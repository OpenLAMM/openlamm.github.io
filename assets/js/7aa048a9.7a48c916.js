"use strict";(self.webpackChunkdyte_docs=self.webpackChunkdyte_docs||[]).push([[2276],{6172:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>s,default:()=>d,frontMatter:()=>l,metadata:()=>u,toc:()=>p});a(7294);var n=a(3905);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function o(e,t){return t=null!=t?t:{},Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):function(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))})),e}function r(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},o=Object.keys(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)a=o[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}const l={},s=void 0,u={type:"mdx",permalink:"/paper_list/LAMM",source:"@site/src/pages/paper_list/LAMM.md",description:"LAMM",frontMatter:{}},p=[{value:"Overview",id:"overview",level:2},{value:"Demos",id:"demos",level:2},{value:"Online Demo",id:"online-demo",level:3},{value:"CLI Demo",id:"cli-demo",level:3},{value:"LAMM-Dataset",id:"lamm-dataset",level:2},{value:"LAMM-Framework",id:"lamm-framework",level:2},{value:"LAMM-Benchmark",id:"lamm-benchmark",level:2},{value:"Citation",id:"citation",level:2},{value:"License",id:"license",level:2},{value:"Acknowledgement",id:"acknowledgement",level:2}],c={toc:p};function d(e){var{components:t}=e,l=r(e,["components"]);return(0,n.kt)("wrapper",o(function(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(e){return Object.getOwnPropertyDescriptor(a,e).enumerable})))),n.forEach((function(t){i(e,t,a[t])}))}return e}({},c,l),{components:t,mdxType:"MDXLayout"}),(0,n.kt)("p",null,(0,n.kt)("img",{alt:"LAMM",src:a(3301).Z,width:"1982",height:"408"})),(0,n.kt)("div",{align:"center"},"Zhenfei Yin",(0,n.kt)("sup",null,"*,1,3"),"\u2003 Jiong Wang",(0,n.kt)("sup",null,"*,1,4"),"\u2003 Jianjian Cao",(0,n.kt)("sup",null,"*,1,4"),"\u2003 Zhelun Shi",(0,n.kt)("sup",null,"*,1,2"),"\u2003 Dingning Liu",(0,n.kt)("sup",null,"1,5"),"\u2003 Mukai Li",(0,n.kt)("sup",null,"1"),"\u2003",(0,n.kt)("br",null),"Xiaoshui Huang",(0,n.kt)("sup",null,"1"),"\u2003 Zhiyong Wang",(0,n.kt)("sup",null,"3"),"\u2003 Lu Sheng",(0,n.kt)("sup",null,"2"),"\u2003 Lei Bai",(0,n.kt)("sup",null,"\u2020,1"),"\u2003 Jing Shao",(0,n.kt)("sup",null,"\u2020,1"),"\u2003 Wanli Ouyang",(0,n.kt)("sup",null,"1")),(0,n.kt)("div",{align:"center"},(0,n.kt)("sup",null,"1"),"Shanghai Artificial Intelligence Laboratory\u2003",(0,n.kt)("sup",null,"2"),"Beihang University\u2003",(0,n.kt)("sup",null,"3"),"The University of Sydney\u2003",(0,n.kt)("br",null),(0,n.kt)("sup",null,"4"),"Fudan University\u2003",(0,n.kt)("sup",null,"5"),"Dalian University of Technology"),(0,n.kt)("div",{align:"center"},(0,n.kt)("sup",null,"*")," Equal Contribution\u2003",(0,n.kt)("sup",null,"\u2020")," Corresponding Authors"),(0,n.kt)("p",{align:"center",style:{paddingTop:"0.75rem"}},(0,n.kt)("font",{size:"4"},(0,n.kt)("a",{href:"https://arxiv.org/pdf/2306.06687.pdf",target:"_blank"},"\ud83d\udcc4 Paper")," \u2022 ",(0,n.kt)("a",{href:"https://openxlab.org.cn/apps/detail/LAMM/LAMM",target:"_blank"},"\ud835\udd4f Demo")," \u2022 ",(0,n.kt)("a",{href:"https://www.youtube.com/watch?v=M7XlIe8hhPk",target:"_blank"},"\u25b6\ufe0f YouTube ")," \u2022 ",(0,n.kt)("a",{href:"https://www.bilibili.com/video/BV1kN411D7kt/?share_source=copy_web&vd_source=ab4c734425ed0114898300f2c037ac0b",target:"_blank"}," \ud83d\udcfa Bilibili ")," \u2022 ",(0,n.kt)("a",{href:"/model_system_card#lamm-models",target:"_blank"},"\ud83d\udce6 LAMM Models"))),(0,n.kt)("h2",{id:"overview"},"Overview"),(0,n.kt)("p",null,"Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities.\nOur main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities' extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research."),(0,n.kt)("h2",{id:"demos"},"Demos"),(0,n.kt)("h3",{id:"online-demo"},"Online Demo"),(0,n.kt)("p",null,"For cases of 2D images, we provide an ",(0,n.kt)("a",{parentName:"p",href:"https://huggingface.co/spaces/openlamm/LAMM"},"online demo")," deployed on huggingface spaces."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"Due to limitation of hardware capacity, online version only supports LLM of 7B parameters and load pretrained model takes few minutes.\n")),(0,n.kt)("h3",{id:"cli-demo"},"CLI Demo"),(0,n.kt)("p",null,"We also provide a CLI demo for local test.\nPoint cloud data are required to be in format of ",(0,n.kt)("inlineCode",{parentName:"p"},"npy"),", we suggest to use data from LAMM-Benchmark-3D."),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-bash"},"    cd ./src\n    python cli_demo.py \\\n        --model lamm_peft \\\n        --vision_type pcl or image \\\n        --encoder_pretrain epcl or clip \\\n        --encoder_ckpt_path $EPCL_CKPT_PATH or '' \\\n        --llm_ckpt_path $LLM_CKPT_PATH \\\n        --delta_ckpt_path $LAMM_CKPT_PATH\n")),(0,n.kt)("h2",{id:"lamm-dataset"},"LAMM-Dataset"),(0,n.kt)("p",null,"LAMM-Dataset is a comprehensive multi-modal instruction tuning dataset, which contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets. Here we design four type of multi-modal instruction-response pairs,"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"C1: n-round daily dialogue focuses on multi-modal daily conversations."),(0,n.kt)("li",{parentName:"ul"},"C2: n-round factual knowledge dialogue aims at factual knowledge reasoning."),(0,n.kt)("li",{parentName:"ul"},"C3: 1-round detailed description aims to elaborate images and 3D scenes in texts."),(0,n.kt)("li",{parentName:"ul"},"C4: 1-round visual task dialogue transfers various vision tasks into instruction-response pairs, aiming at enhancing generalizability towards domain tasks in other modalities.")),(0,n.kt)("p",null,"You can download ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/datasets/instruction"},"instruction")," / ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/datasets/benchmark"},"benchmark")," dataset and put them into ",(0,n.kt)("inlineCode",{parentName:"p"},"data/LAMM")," directory."),(0,n.kt)("h2",{id:"lamm-framework"},"LAMM-Framework"),(0,n.kt)("ol",null,(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"You can install the environment following ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/installation#training"},"here"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Prepare the required pretrained weights of LLMs and visual encoder ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/training"},"here"),".")),(0,n.kt)("li",{parentName:"ol"},(0,n.kt)("p",{parentName:"li"},"Train your LAMM model following ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/training"},"here"),". We also provide pretrained model ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/training"},"here"),"."))),(0,n.kt)("h2",{id:"lamm-benchmark"},"LAMM-Benchmark"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Note"),": We highly recommend you use ChEF to evalute LAMM model, see ",(0,n.kt)("a",{parentName:"p",href:"/tutorial/benchmark/default"},"here")," for details."),(0,n.kt)("p",null,"Default LAMM-Benchmark evaluates 9 common image tasks, using a total of 11 datasets with over ",(0,n.kt)("strong",{parentName:"p"},"62,439")," samples, and 3 common point cloud tasks, by utilizing 3 datasets with over ",(0,n.kt)("strong",{parentName:"p"},"12,788")," data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies."),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"We are the very first attempt to establish a benchmark for MLLMs. We conducted a comprehensive benchmark to quantify the zero-shot and fine-tuning performance of existing multi-modal language models on various computer vision tasks and compare them against state-of-the-art methods of these tasks, including classification, object detection, pose estimation, visual question answering, facial classification, optical character recognition, object counting.")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("p",{parentName:"li"},"We also attempted two novel evaluation strategies designed explicitly for MLLMs. Specifically, as for text generation, we established a scoring logic based on the GPT API. As for tasks involving interactions between points and images, such as object detection and pose estimation, we proposed an object-locating evaluation method."))),(0,n.kt)("h2",{id:"citation"},"Citation"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"@article{yin2023lamm,\n    title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},\n    author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},\n    journal={arXiv preprint arXiv:2306.06687},\n    year={2023}\n}\n")),(0,n.kt)("h2",{id:"license"},"License"),(0,n.kt)("p",null,"The project is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The checkpoints are also CC BY NC 4.0 (allowing only non-commercial use)."),(0,n.kt)("h2",{id:"acknowledgement"},"Acknowledgement"),(0,n.kt)("p",null,"We thank ",(0,n.kt)("a",{parentName:"p",href:"https://scholar.google.com/citations?user=Wnk95ccAAAAJ"},"Hongxing Fan"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/Zx55"},"Zeren Chen"),", Zhen Wang for support of LAMM project."),(0,n.kt)("p",null,"We also thanks the great works including ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/openai/CLIP"},"CLIP"),", ",(0,n.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2212.04098"},"EPCL"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/llama"},"LLaMA"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/lm-sys/FastChat"},"Vicuna"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/Dao-AILab/flash-attention/"},"FlashAttention"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/facebookresearch/xformers"},"xformers"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/ModelTC/lightllm"},"lightllm")))}d.isMDXComponent=!0},3301:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/lamm-title-235ebf88edc0c0022cbfb2c69f98ff30.png"}}]);