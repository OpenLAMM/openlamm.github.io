"use strict";(self.webpackChunkdyte_docs=self.webpackChunkdyte_docs||[]).push([[8637],{6904:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>u,toc:()=>p});a(7294);var n=a(3905);function i(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function r(t,e){return e=null!=e?e:{},Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(e)):function(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}(Object(e)).forEach((function(a){Object.defineProperty(t,a,Object.getOwnPropertyDescriptor(e,a))})),t}function l(t,e){if(null==t)return{};var a,n,i=function(t,e){if(null==t)return{};var a,n,i={},r=Object.keys(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||(i[a]=t[a]);return i}(t,e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(n=0;n<r.length;n++)a=r[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(i[a]=t[a])}return i}const o={title:"Instruction Tuning",sidebar_position:1},s=void 0,u={unversionedId:"Datasets/instruction",id:"Datasets/instruction",title:"Instruction Tuning",description:"We construct instruction tuning dataset for 2D/3D modality instruction tuning via GPT API.",source:"@site/docs/tutorial/Datasets/instruction.mdx",sourceDirName:"Datasets",slug:"/Datasets/instruction",permalink:"/tutorial/Datasets/instruction",draft:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{title:"Instruction Tuning",sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Datasets",permalink:"/tutorial/Datasets/"},next:{title:"Benchmarking",permalink:"/tutorial/Datasets/benchmark"}},c={},p=[{value:"2D Instruction Tuning Datasets",id:"2d-instruction-tuning-datasets",level:2},{value:"Additional Detection Instruction",id:"additional-detection-instruction",level:4},{value:"3D Instruction Tuning Datasets",id:"3d-instruction-tuning-datasets",level:2},{value:"Scan2Inst",id:"scan2inst",level:4},{value:"LAMM3D-Dataset",id:"lamm3d-dataset",level:4},{value:"Directory Structure",id:"directory-structure",level:2},{value:"Meta File Format",id:"meta-file-format",level:2}],d={toc:p};function m(t){var{components:e}=t,a=l(t,["components"]);return(0,n.kt)("wrapper",r(function(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{},n=Object.keys(a);"function"==typeof Object.getOwnPropertySymbols&&(n=n.concat(Object.getOwnPropertySymbols(a).filter((function(t){return Object.getOwnPropertyDescriptor(a,t).enumerable})))),n.forEach((function(e){i(t,e,a[e])}))}return t}({},d,a),{components:e,mdxType:"MDXLayout"}),(0,n.kt)("p",null,"We construct instruction tuning dataset for 2D/3D modality instruction tuning via GPT API."),(0,n.kt)("h2",{id:"2d-instruction-tuning-datasets"},"2D Instruction Tuning Datasets"),(0,n.kt)("p",null,"2D instruction tuning datasets are build on ",(0,n.kt)("a",{parentName:"p",href:"https://cocodataset.org"},"MS-COCO"),", ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/ZhangYuanhan-AI/Bamboo"},"Bamboo"),", ",(0,n.kt)("a",{parentName:"p",href:"https://isrc.iscas.ac.cn/gitlab/research/locount-dataset"},"Locount")," and ",(0,n.kt)("a",{parentName:"p",href:"https://textvqa.org"},"TextVQA")," dataset. You can download them from ",(0,n.kt)("a",{parentName:"p",href:"https://opendatalab.com/LAMM/LAMM/tree/main/raw/2D_Instruct"},"here"),"."),(0,n.kt)("p",null,"The generated instruction-following dialogues are organized into the following meta files. We provide a table to illustrate the correspondence between each meta file and data collection:"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"),(0,n.kt)("th",{parentName:"tr",align:null},"Data file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/daily_dialogue_49k.json"},"daily_dialogue_49k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"112M"),(0,n.kt)("td",{parentName:"tr",align:null},"coco_images.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"7.8G")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/detailed_description_49k.json"},"detailed_description_49k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"65.5M"),(0,n.kt)("td",{parentName:"tr",align:null},"coco_images.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"7.8G")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/factual_knowledge_dialogue_42k.json"},"factual_knowledge_dialogue_42k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"83.2M"),(0,n.kt)("td",{parentName:"tr",align:null},"bamboo_images.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"5.4G")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/vision_task_dialogue_46k.json"},"vision_task_dialogue_46k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"64.8M"),(0,n.kt)("td",{parentName:"tr",align:null},"coco_images.zip, bamboo_images.zip, locount_images.zip, textvqa_images.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"9.2G")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/LAMM_instruct_186k.json"},"LAMM_instruct_186k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"325M"),(0,n.kt)("td",{parentName:"tr",align:null},"-"),(0,n.kt)("td",{parentName:"tr",align:null},"-")))),(0,n.kt)("p",null,"Note that we provide a ",(0,n.kt)("inlineCode",{parentName:"p"},"LAMM_instruct_186k.json")," meta file to merge all the dataset across different tasks. You can just use this file for training."),(0,n.kt)("h4",{id:"additional-detection-instruction"},"Additional Detection Instruction"),(0,n.kt)("p",null,"The lack of sufficient detection instructions results in poor performance on downstream PASCAL VOC evaluation. To overcome this problem, we leverage entire COCO detection annotations to generate instructions, and add them into the aforementioned datasets as supplementation."),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"),(0,n.kt)("th",{parentName:"tr",align:null},"Data file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://opendatalab.com/LAMM/OctaviusDataset/tree/main/OctaviusDataset_2D/meta_file"},"coco_detection_117k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"116M"),(0,n.kt)("td",{parentName:"tr",align:null},"coco_images.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"7.8G")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://opendatalab.com/LAMM/OctaviusDataset/tree/main/OctaviusDataset_2D/meta_file"},"octavius_2d_train_293k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"399M"),(0,n.kt)("td",{parentName:"tr",align:null},"-"),(0,n.kt)("td",{parentName:"tr",align:null},"-")))),(0,n.kt)("p",null,"Note that we provide a ",(0,n.kt)("inlineCode",{parentName:"p"},"octavius_2d_train_293k.json")," meta file to merge all the dataset across different tasks. You can just use this file for training."),(0,n.kt)("h2",{id:"3d-instruction-tuning-datasets"},"3D Instruction Tuning Datasets"),(0,n.kt)("p",null,'We provide two 3D instruction tuning datasets, "Scan2Inst" and "LAMM3D-Dataset", for 3D instruction tuning.'),(0,n.kt)("h4",{id:"scan2inst"},"Scan2Inst"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"Note: Compared with LAMM3D-Dataset, Scan2Inst provide more tasks and instruction-following dialogues, therefore we highly recommend you use Scan2Inst rather than LAMM3D-Dataset for 3D instruction tuning.")),(0,n.kt)("p",null,"Scan2Inst is build on ",(0,n.kt)("a",{parentName:"p",href:"http://www.scan-net.org"},"ScanNet"),". Specifically, we first use FCAF3D from ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/open-mmlab/mmdetection"},"mmdetection3d")," to extract 3d object given a scene level point cloud. Then a ULIP-like encoder is used to extract linguistic-aligned object level 3d feature. In the end, to speed up the data loading process, we store the dataset to a pickle file. For convincely, we provide a processed pickle file (scan2inst_train.pickle) ",(0,n.kt)("a",{parentName:"p",href:"https://opendatalab.com/LAMM/OctaviusDataset/tree/main/OctaviusDataset_3D/3D_Instruct/meta_file"},"here"),", you can train our model by just loading this file."),(0,n.kt)("p",null,"Besides, if you want to utilize your own dataset, we also provide our ULIP model pretraining code, you can train your own ULIP model by following the instructions from ",(0,n.kt)("inlineCode",{parentName:"p"},"src/tools/Octavius/ULIP/scripts/pretrain_pointbert.sh"),". You can also use our pretrained model ",(0,n.kt)("a",{parentName:"p",href:"https://drive.google.com/file/d/1-0Z3Q3Z3Z3Z3Z3Z3Z3Z3Z3Z3Z3Z3Z3Z/view?usp=sharing"},"Here")," to extract your own dataset."),(0,n.kt)("p",null,"Corresponding meta file is here:"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"),(0,n.kt)("th",{parentName:"tr",align:null},"Data file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://opendatalab.com/LAMM/OctaviusDataset/tree/main/OctaviusDataset_3D/3D_Instruct/meta_file"},"scan2inst_train.json")),(0,n.kt)("td",{parentName:"tr",align:null},"62.3M"),(0,n.kt)("td",{parentName:"tr",align:null},"scan2inst_train.pickle"),(0,n.kt)("td",{parentName:"tr",align:null},"209M")))),(0,n.kt)("h4",{id:"lamm3d-dataset"},"LAMM3D-Dataset"),(0,n.kt)("p",null,"LAMM3D-Dataset is build on ",(0,n.kt)("a",{parentName:"p",href:"https://github.com/WaldJohannaU/3RScan"},"3RScan")," and ",(0,n.kt)("a",{parentName:"p",href:"https://shapenet.org"},"ShapeNet"),". You can download them from ",(0,n.kt)("a",{parentName:"p",href:"https://opendatalab.com/LAMM/LAMM/tree/main/raw/3D_Instruct"},"here"),"."),(0,n.kt)("p",null,"Corresponding meta file is here:"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"),(0,n.kt)("th",{parentName:"tr",align:null},"Data file name"),(0,n.kt)("th",{parentName:"tr",align:null},"Size"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",{parentName:"tr",align:null},(0,n.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/3D_Instruct/meta_file/LAMM_3dinstruct_10k.json"},"LAMM_3dinstruct_10k.json")),(0,n.kt)("td",{parentName:"tr",align:null},"19.6M"),(0,n.kt)("td",{parentName:"tr",align:null},"3rscan_pcls.zip, shapenet_pcls.zip"),(0,n.kt)("td",{parentName:"tr",align:null},"929M")))),(0,n.kt)("h2",{id:"directory-structure"},"Directory Structure"),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre"},"data\n\u251c\u2500\u2500 LAMM\n\u2502   \u251c\u2500\u2500 2D_Instruct  \n\u2502   \u2502   \u251c\u2500\u2500 coco_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 bamboo_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 textvqa_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 locount_images.zip  \n\u2502   \u2502   \u2514\u2500\u2500 meta_file  \n\u2502   \u2502       \u251c\u2500\u2500 daily_dialogue_49k.json  \n\u2502   \u2502       \u251c\u2500\u2500 detailed_description_49k.json  \n\u2502   \u2502       \u251c\u2500\u2500 factual_knowledge_dialogue_42k.json  \n\u2502   \u2502       \u2514\u2500\u2500 vision_task_dialogue_46k.json \n\u2502   \u251c\u2500\u2500 3D_Instruct\n\u2502   \u2502   \u251c\u2500\u2500 3rscan_pcls.zip  \n\u2502   \u2502   \u251c\u2500\u2500 shapenet_pcls.zip  \n\u2502   \u2502   \u2514\u2500\u2500 meta_file  \n\u2502   \u2502       \u2514\u2500\u2500 LAMM_3dinstruct_10k.json\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 Octavius\n\u2502   \u251c\u2500\u2500 2D_Instruct  \n\u2502   \u2502   \u251c\u2500\u2500 coco_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 bamboo_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 textvqa_images.zip  \n\u2502   \u2502   \u251c\u2500\u2500 locount_images.zip  \n\u2502   \u2502   \u2514\u2500\u2500 meta_file  \n\u2502   \u2502       \u2514\u2500\u2500 octavius_2d_train_293k.json\n\u2502   \u251c\u2500\u2500 3D_Instruct  \n\u2502   \u2502   \u251c\u2500\u2500 scan2inst_train.pickle\n\u2502   \u2502   \u2514\u2500\u2500 meta_file  \n\u2502   \u2502       \u2514\u2500\u2500 scan2inst_train.json  \n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u2514\u2500\u2500 ...\n")),(0,n.kt)("h2",{id:"meta-file-format"},"Meta File Format"),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"2D instruction tuning data")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-json"},'[\n    {\n    "id": "000000019028",  # image id\n    "image": "coco_images/000000019028.jpg", # image path\n    "conversations": [\n        {\n            "from": "human",  # instruction\n            "value": "How is the kitchen in the image furnished?"\n        },\n        {\n            "from": "gpt",  # response\n            "value": "The kitchen in the image is furnished with white cabinets and white   appliances. There is a dishwasher, a stove, and a sink. On the stove, a blue towel    hangs on the handle. A cutting board is placed on the dishwasher. There are also   additional elements like a bowl of apples on the counter and a beige rug on the floor."\n        }\n    ],\n    "task_type": "conversation",  # task type\n    "src_image": "coco2017" # original dataset\n    },\n    {\n        ...\n    }\n]\n')),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"3D instruction instruction data")),(0,n.kt)("pre",null,(0,n.kt)("code",{parentName:"pre",className:"language-json"},'[\n    {\n        "pcl": "shapenet_pcls/04256520_cb71cb7b36dbcb6f826fc8d57346a2e4_4096.npy",\n        "conversations": [\n                {\n                    "from": "human",\n                    "value": "What scenario does this point cloud belong to according to the    model\\u2019s prediction?"\n                },\n                {\n                    "from": "gpt",\n                    "value": "Through meticulous analysis, it becomes evident that the point cloud  aligns with the characteristics of sofa,couch,lounge s       cenario."\n                }\n            ],\n        "task_type": "classification3d",\n        "src_dataset": "ShapeNet",\n        "src_id": "04256520_cb71cb7b36dbcb6f826fc8d57346a2e4"\n    },\n    {\n        ...\n    }\n]\n')))}m.isMDXComponent=!0}}]);