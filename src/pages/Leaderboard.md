# Leaderboard


## Visual performance of MLLMs on different Scenarios
| **Scenario**      | **CIFAR** | **Flickr** | **VOC** | **Omniâ€ ** | **FSC** | **SQA** | **MM** | **SEED** | **MME** |
|---------------|--------|--------|-------|-----------|-------|-------|-------|-------|-------|
| **Question Type** | gen.   | gen.  | gen.  | gen.      | gen.  | discrim. | discrim. | gen.  | discrim. |
| **LLaVA**         | **89.40** | 80.80  | 26.01 | 26.62     | 24.11 | 46.55    | 43.13   | 46.45 | 50.17  |
| **LAMM**          | 80.70  | 72.50  | 29.58 | 22.54     | 19.33 | 52.75    | 44.47   | 47.03 | 55.82  |
| **MiniGPT-4**     | 80.80  | 71.50  | 26.51 | 30.60     | 22.52 | 47.0     | 54.34 | 46.48 | 57.12  |
| **mPLUG**         | 79.67  | 79.20  | 28.50 | 30.70     | 20.92 | 48.44    | 49.57   | 42.81 | 71.59 |
| **Otter**         | 81.34  | 71.30  | 27.15 | 26.41     | 20.00 | 50.22    | 53.91   | 36.40 | 63.78  |
| **LAv2**          | 70.17  | 79.50  | 31.60 | **32.00** | 21.26 | 54.34 | 57.06 | 35.41 | 69.90  |
| **InstructBLIP**  | 84.27  | 79.40 | 27.65 | 30.75    | **25.04** | **55.18**    | **65.73**  | **50.81** | **72.0**   |
| **Shikra**        | 68.71  | **94.70**  | **55.23** | 22.89   | 22.43 | 45.21    | 63.26  | 49.79 | 70.28  |
| **Kosmos-2**      | 88.87  | 85.70  | 54.55 | 21.34     | 21.93 | 34.60    | 25.60  | 46.38 | 52.95  |
| **Random Choice** | 10.0   | 25.00  | 25.00 | 10.94     | 20.00 | 35.80    | 27.57  | 24.27 | 50.00  |

    
## Results of Desiderata

| **Desiderata**      | **Calibration** | **In-context Learning** | **Instruction Following** | **Language Performance** | **Hallucination** | **Robustness** |
|---------------|--------|--------|-------|-----------|-------|-------|
| **LLaVA**         |  |  |  |  |  |  |
| **LAMM**          |  |  |  |  |  |  |
| **MiniGPT-4**     |  |  |  |  |  |  |
| **mPLUG**         |  |  |  |  |  |  |
| **Otter**         |  |  |  |  |  |  |
| **LAv2**          |  |  |  |  |  |  |
| **InstructBLIP**  |  |  |  |  |  |  |
| **Shikra**        |  |  |  |  |  |  |
| **Kosmos-2**      |  |  |  |  |  |  |
| **Random Choice** |  |  |  |  |  |  |


## Comparison among GPT-4V, Bard, LLaVA, Otter and MiniGPT-4
| **MLLM**       | **ScienceQA** | **MMBench** | **In-context Learning**    | **Instruction Following** | **Robustness** | **Hallucination** |
|-------------|-----------|---------|--------|--------------|------------|---------------|
| **GPT-4V**  | **96.67** | **93.80** | 43.98  | **97.69**    | **82.16**  | **96.00**        |
| **Bard**    | 90.00     | 71.43    | 39.61  | 71.41        | 71.05       | 88.88           |
| **LLaVA**   | 50.00     | 43.33    | **47.99** | 36.67        | 34.18       | 36.67             |
| **Otter**   | 63.33     | 50.00    | 47.91  | 44.44        | 37.35       | 80.00             |
| **mPLUG-Owl**| 53.33    | 46.67    | 42.14  | 41.67        | 63.46       | 36.67            |