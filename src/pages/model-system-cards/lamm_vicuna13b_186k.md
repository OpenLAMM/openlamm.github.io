---
---

# lamm_13b_lora32_186k

<!-- Provide a quick summary of what the model is/does. [Optional] -->

MultiModal Large Language Model finetuned on 186K image-text pairs from LAMM Dataset, towards unified vision-language understanding.
This is LoRA weights based on Vicuna-v0-13b and you should use it as delta model.

# Table of Contents

- [Model Details](#model-details)
  - [Model Description](#model-description)
  - [Model Configuration](#model-configuration)
- [Uses](#uses)
  - [Download link](#download-links)
  - [Direct Use](#direct-use)
- [Bias, Risks, and Limitations](#bias-risks-and-limitations)
  - [Recommendations](#recommendations)
- [Training Details](#training-details)
  - [Training Data](#training-data)
  - [Training Procedure](#training-procedure)
    - [Preprocessing](#preprocessing)
    <!-- - [Speeds, Sizes, Times](#speeds-sizes-times) -->
- [Evaluation](#evaluation)
  - [Testing Data](#testing-data-factors--metrics)
    - [Testing Data](#testing-data)
      <!-- - [Factors](#factors) -->
      <!-- - [Metrics](#metrics) -->
  - [Results](#results)
  <!-- - [Model Examination](#model-examination) -->
- [Environmental Impact](#environmental-impact)
- [Technical Specifications [optional]](#technical-specifications-optional)
  - [Model Architecture and Objective](#model-architecture-and-objective)
  - [Compute Infrastructure](#compute-infrastructure)
    - [Hardware](#hardware)
    - [Software](#software)
- [Citation](#citation)
- [Model Card Contact](#model-card-contact)
- [How to Get Started with the Model](#how-to-get-started-with-the-model)

---

# Model Details

## Model Description

<!-- Provide a longer summary of what this model is/does. -->

MultiModal Large Language Model finetuned on 186K image-text pairs from LAMM Dataset. This model consists of feature projector and LoRA weight finetuned on LLM.

- **Developed by:** Shanghai Artificial Intelligence Laboratory
- **Shared by [Optional]:** LAMM Team
- **Model type:** Language model
- **Language(s) (NLP):** en
- **License:** cc-by-nc-4.0
- **Parent Model:** Vicuna_v0 13B
- **Resources for more information:**
  - [GitHub Repo: LAMM](https://github.com/OpenLAMM/LAMM/)
  - [Associated Paper](https://arxiv.org/abs/2306.06687)

## Model Configuration

- **LLM**: Vicuna_13b_v0
- **Vision Encoder**: CLIP ViT-L-14
- **lora_r**: 32
- **lora_alpha**: 32
- **lora_dropout**: 0.1
- **lora_target_modules**: ['q_proj', 'k_proj', 'v_proj', 'o_proj']

# Uses

<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->

## Download links

- [Huggingface](https://huggingface.co/openlamm/lamm_13b_lora32_186k)
- [OpenXLab](https://openxlab.org.cn/models)

## Direct Use

```python
llm_ckpt_path = "/PATH/TO/VICUNA/MODEL"
peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=args["lora_r"],
            lora_alpha=args["lora_alpha"],
            lora_dropout=args["lora_dropout"],
            target_modules=args["lora_target_modules"],
        )
llama_model = LlamaForCausalLM.from_pretrained(llm_ckpt_path)
llama_model = get_peft_model(llama_model, peft_config)
```

<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->

<!-- ## Downstream Use [Optional] -->

<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->

<!-- ## Out-of-Scope Use -->

<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->
<!-- If the user enters content, print that. If not, but they enter a task in the list, use that. If neither, say "more info needed." -->

# Bias, Risks, and Limitations

<!-- This section is meant to convey both technical and sociotechnical limitations. -->

Significant research has explored bias and fairness issues with language models (see, e.g., [Sheng et al. (2021)](https://aclanthology.org/2021.acl-long.330.pdf) and [Bender et al. (2021)](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)). Predictions generated by the model may include disturbing and harmful stereotypes across protected classes; identity characteristics; and sensitive, social, and occupational groups.

## Recommendations

<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->

# Training Details

## Training Data

<!-- This should link to a Data Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->

This model is trained on LAMM-Dataset-186K.

## Training Procedure

<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->

The training process is conducted in one step. The whole framework is trained end-to-end, and objective is to predict next-token.

### Preprocessing

Training data should be in form of conversations round by round, which is shown as follow:

```json
{
  'image': "/PATH/TO/IMAGE",
  'conversation': [
    {
      'from': 'human',
      'value': '<img>QUESTION ABOUT THE GIVEN IMAGE',
    },
    {
      'from': 'assistant',
      'value': 'ANSWER TO HUMAN'.
    },
    ...
  ]
}
```

### Speeds, Sizes, Times

<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->

The training process lasts for about 8 hours on 4 A100-80GB in precision of bf16.

# Evaluation

<!-- This section describes the evaluation protocols and provides the results. -->

## Testing Data, Factors & Metrics

### Testing Data

<!-- This should link to a Data Card if possible. -->

Model performance is evaluated on LAMM-Benchmark-2D. Test is in zero-shot setting.

<!-- ### Factors -->

<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->

<!-- More information needed -->

<!-- ### Metrics -->

<!-- These are the evaluation metrics being used, ideally with a description of why. -->

<!-- More information needed -->

## Results

| Dataset | CIFAR10 | VOC2012 | SQAimage | flickr30k | UCMerced | FSC147 |   SVT    | CelebA-Smile | CelebA-Hair |  LSP   |
| :-----: | :-----: | :-----: | :------: | :-------: | :------: | :----: | :------: | :----------: | :---------: | :----: |
| Metrics |   Acc   |   mAP   |   Acc    |   BLEU4   |   Acc    |  MAE   | Word Acc |     Acc      |     Acc     |  PCK   |
| Result  |  37.9   |  7.20   |  49.88   |   2.56    |  18.23   | 46.88  |  29.14   |    57.60     |    56.96    | Failed |

<!-- # Model Examination

More information needed -->

# Environmental Impact

32 A100 GPU hours is equivalent to 3.46kg CO2eq.

<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->

Carbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).

- **Hardware Type:** A100 80GB
- **Hours used:** 32 GPU Hours
- **Cloud Provider:** Private Infrastructure
- **Compute Region:** China
- **Carbon Emitted:** 3.46

# Technical Specifications

## Model Architecture and Objective

Details can be found in the associated paper.

### Hardware

We trained the model on 4 A100-80GB. You can also run with V100 / RTX3090.

### Software

The training framework is built on PyTorch & transformers.

# Citation

<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->

**BibTeX:**

```
@article{yin2023lamm,
title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},
journal={arXiv preprint arXiv:2306.06687},
year={2023}
}
```

# Model Card Contact

openlamm@gmail.com

<!--
# How to Get Started with the Model

Use the code below to get started with the model.

<details>
<summary> Click to expand </summary>

More information needed

</details> -->
