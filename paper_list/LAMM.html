<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.1">
<title data-rh="true">LAMM</title><meta data-rh="true" property="og:title" content="LAMM"><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://openlamm.github.io/logo/LAMM-logo.png"><meta data-rh="true" name="twitter:image" content="https://openlamm.github.io/logo/LAMM-logo.png"><meta data-rh="true" property="og:url" content="https://openlamm.github.io/paper_list/LAMM"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" name="description" content="LAMM"><meta data-rh="true" property="og:description" content="LAMM"><link data-rh="true" rel="icon" href="/logo/LAMM-logo.png"><link data-rh="true" rel="canonical" href="https://openlamm.github.io/paper_list/LAMM"><link data-rh="true" rel="alternate" href="https://openlamm.github.io/paper_list/LAMM" hreflang="en"><link data-rh="true" rel="alternate" href="https://openlamm.github.io/paper_list/LAMM" hreflang="x-default"><link data-rh="true" rel="stylesheet" href="/assets/css/docsly.min.css"><link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n,g){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var m=t.getElementsByTagName(a)[0],r=t.createElement(a);r.async=!0,r.src="https://www.googletagmanager.com/gtm.js?id=GTM-5FDFFSS",m.parentNode.insertBefore(r,m)}(window,document,"script","dataLayer")</script>





<script src="https://cdn.statuspage.io/se-v2.js"></script><link rel="stylesheet" href="/assets/css/styles.959e90f6.css">
<link rel="preload" href="/assets/js/runtime~main.b10b5ab7.js" as="script">
<link rel="preload" href="/assets/js/main.d6ae70d1.js" as="script">
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5FDFFSS" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>

<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/logo/LAMM-logo-light.png" alt="LAMM" class="themedImage_ToTc themedImage--light_HNdA" height="60px"><img src="/logo/LAMM-logo-dark.png" alt="LAMM" class="themedImage_ToTc themedImage--dark_i4oU" height="60px"></div></a><a class="navbar__item navbar__link" href="/tutorial">Tutorial</a><a class="navbar__item navbar__link" href="/datasets">Datasets</a><a class="navbar__item navbar__link" href="/model_system_card">Models</a><a class="navbar__item navbar__link" href="/Leaderboards">Leaderboards</a><a class="navbar__item navbar__link" href="/Team">Team</a><a href="https://github.com/OpenGVLab/LAMM" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Github</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><p><img loading="lazy" alt="LAMM" src="/assets/images/lamm-title-235ebf88edc0c0022cbfb2c69f98ff30.png" width="1982" height="408" class="img_ev3q"></p><div align="center">Zhenfei Yin<sup>*,1,3</sup>  Jiong Wang<sup>*,1,4</sup>  Jianjian Cao<sup>*,1,4</sup>  Zhelun Shi<sup>*,1,2</sup>  Dingning Liu<sup>1,5</sup>  Mukai Li<sup>1</sup> <br>Xiaoshui Huang<sup>1</sup>  Zhiyong Wang<sup>3</sup>  Lu Sheng<sup>2</sup>  Lei Bai<sup>†,1</sup>  Jing Shao<sup>†,1</sup>  Wanli Ouyang<sup>1</sup></div><div align="center"><sup>1</sup>Shanghai Artificial Intelligence Laboratory <sup>2</sup>Beihang University <sup>3</sup>The University of Sydney <br><sup>4</sup>Fudan University <sup>5</sup>Dalian University of Technology</div><div align="center"><sup>*</sup> Equal Contribution <sup>†</sup> Corresponding Authors</div><p align="center" style="padding-top:0.75rem"><font size="4"><a href="https://arxiv.org/pdf/2306.06687.pdf" target="_blank" rel="noopener noreferrer">📄 Paper</a> • <a href="https://openxlab.org.cn/apps/detail/LAMM/LAMM" target="_blank" rel="noopener noreferrer">𝕏 Demo</a> • <a href="https://www.youtube.com/watch?v=M7XlIe8hhPk" target="_blank" rel="noopener noreferrer">▶️ YouTube </a> • <a href="https://www.bilibili.com/video/BV1kN411D7kt/?share_source=copy_web&amp;vd_source=ab4c734425ed0114898300f2c037ac0b" target="_blank" rel="noopener noreferrer"> 📺 Bilibili </a> • <a target="_blank" href="/model_system_card#lamm-models">📦 LAMM Models</a></font></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overview">Overview<a href="#overview" class="hash-link" aria-label="Direct link to Overview" title="Direct link to Overview">​</a></h2><p>Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities.
Our main contribution is three-fold: 1) We present the LAMM-Dataset and LAMM-Benchmark, which cover almost all high-level vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We demonstrate the detailed methods of constructing instruction-tuning datasets and benchmarks for MLLMs, which will enable future research on MLLMs to scale up and extend to other domains, tasks, and modalities faster. 3) We provide a primary but potential MLLM training framework optimized for modalities&#x27; extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="demos">Demos<a href="#demos" class="hash-link" aria-label="Direct link to Demos" title="Direct link to Demos">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="online-demo">Online Demo<a href="#online-demo" class="hash-link" aria-label="Direct link to Online Demo" title="Direct link to Online Demo">​</a></h3><p>For cases of 2D images, we provide an <a href="https://huggingface.co/spaces/openlamm/LAMM" target="_blank" rel="noopener noreferrer">online demo</a> deployed on huggingface spaces.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Due to limitation of hardware capacity, online version only supports LLM of 7B parameters and load pretrained model takes few minutes.</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cli-demo">CLI Demo<a href="#cli-demo" class="hash-link" aria-label="Direct link to CLI Demo" title="Direct link to CLI Demo">​</a></h3><p>We also provide a CLI demo for local test.
Point cloud data are required to be in format of <code>npy</code>, we suggest to use data from LAMM-Benchmark-3D.</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token builtin class-name" style="color:rgb(255, 203, 107)">cd</span><span class="token plain"> ./src</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    python cli_demo.py </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --model lamm_peft </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --vision_type pcl or image </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --encoder_pretrain epcl or clip </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --encoder_ckpt_path </span><span class="token variable" style="color:rgb(191, 199, 213)">$EPCL_CKPT_PATH</span><span class="token plain"> or </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;&#x27;</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --llm_ckpt_path </span><span class="token variable" style="color:rgb(191, 199, 213)">$LLM_CKPT_PATH</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        --delta_ckpt_path </span><span class="token variable" style="color:rgb(191, 199, 213)">$LAMM_CKPT_PATH</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lamm-dataset">LAMM-Dataset<a href="#lamm-dataset" class="hash-link" aria-label="Direct link to LAMM-Dataset" title="Direct link to LAMM-Dataset">​</a></h2><p>LAMM-Dataset is a comprehensive multi-modal instruction tuning dataset, which contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets. Here we design four type of multi-modal instruction-response pairs,</p><ul><li>C1: n-round daily dialogue focuses on multi-modal daily conversations.</li><li>C2: n-round factual knowledge dialogue aims at factual knowledge reasoning.</li><li>C3: 1-round detailed description aims to elaborate images and 3D scenes in texts.</li><li>C4: 1-round visual task dialogue transfers various vision tasks into instruction-response pairs, aiming at enhancing generalizability towards domain tasks in other modalities.</li></ul><p>You can download <a href="/tutorial/datasets/instruction">instruction</a> / <a href="/tutorial/datasets/benchmark">benchmark</a> dataset and put them into <code>data/LAMM</code> directory.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lamm-framework">LAMM-Framework<a href="#lamm-framework" class="hash-link" aria-label="Direct link to LAMM-Framework" title="Direct link to LAMM-Framework">​</a></h2><ol><li><p>You can install the environment following <a href="/tutorial/installation#training">here</a>.</p></li><li><p>Prepare the required pretrained weights of LLMs and visual encoder <a href="/tutorial/training">here</a>.</p></li><li><p>Train your LAMM model following <a href="/tutorial/training">here</a>. We also provide pretrained model <a href="/tutorial/training">here</a>.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="lamm-benchmark">LAMM-Benchmark<a href="#lamm-benchmark" class="hash-link" aria-label="Direct link to LAMM-Benchmark" title="Direct link to LAMM-Benchmark">​</a></h2><p><strong>Note</strong>: We highly recommend you use ChEF to evalute LAMM model, see <a href="/tutorial/benchmark/default">here</a> for details.</p><p>Default LAMM-Benchmark evaluates 9 common image tasks, using a total of 11 datasets with over <strong>62,439</strong> samples, and 3 common point cloud tasks, by utilizing 3 datasets with over <strong>12,788</strong> data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies.</p><ul><li><p>We are the very first attempt to establish a benchmark for MLLMs. We conducted a comprehensive benchmark to quantify the zero-shot and fine-tuning performance of existing multi-modal language models on various computer vision tasks and compare them against state-of-the-art methods of these tasks, including classification, object detection, pose estimation, visual question answering, facial classification, optical character recognition, object counting.</p></li><li><p>We also attempted two novel evaluation strategies designed explicitly for MLLMs. Specifically, as for text generation, we established a scoring logic based on the GPT API. As for tasks involving interactions between points and images, such as object detection and pose estimation, we proposed an object-locating evaluation method.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="citation">Citation<a href="#citation" class="hash-link" aria-label="Direct link to Citation" title="Direct link to Citation">​</a></h2><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">@article{yin2023lamm,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    title={LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    journal={arXiv preprint arXiv:2306.06687},</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    year={2023}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h2 class="anchor anchorWithStickyNavbar_LWe7" id="license">License<a href="#license" class="hash-link" aria-label="Direct link to License" title="Direct link to License">​</a></h2><p>The project is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The checkpoints are also CC BY NC 4.0 (allowing only non-commercial use).</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="acknowledgement">Acknowledgement<a href="#acknowledgement" class="hash-link" aria-label="Direct link to Acknowledgement" title="Direct link to Acknowledgement">​</a></h2><p>We thank <a href="https://scholar.google.com/citations?user=Wnk95ccAAAAJ" target="_blank" rel="noopener noreferrer">Hongxing Fan</a>, <a href="https://github.com/Zx55" target="_blank" rel="noopener noreferrer">Zeren Chen</a>, Zhen Wang for support of LAMM project.</p><p>We also thanks the great works including <a href="https://github.com/openai/CLIP" target="_blank" rel="noopener noreferrer">CLIP</a>, <a href="https://arxiv.org/abs/2212.04098" target="_blank" rel="noopener noreferrer">EPCL</a>, <a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener noreferrer">LLaMA</a>, <a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener noreferrer">Vicuna</a>, <a href="https://github.com/Dao-AILab/flash-attention/" target="_blank" rel="noopener noreferrer">FlashAttention</a>, <a href="https://github.com/facebookresearch/xformers" target="_blank" rel="noopener noreferrer">xformers</a>, <a href="https://github.com/ModelTC/lightllm" target="_blank" rel="noopener noreferrer">lightllm</a></p></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview" class="table-of-contents__link toc-highlight">Overview</a></li><li><a href="#demos" class="table-of-contents__link toc-highlight">Demos</a><ul><li><a href="#online-demo" class="table-of-contents__link toc-highlight">Online Demo</a></li><li><a href="#cli-demo" class="table-of-contents__link toc-highlight">CLI Demo</a></li></ul></li><li><a href="#lamm-dataset" class="table-of-contents__link toc-highlight">LAMM-Dataset</a></li><li><a href="#lamm-framework" class="table-of-contents__link toc-highlight">LAMM-Framework</a></li><li><a href="#lamm-benchmark" class="table-of-contents__link toc-highlight">LAMM-Benchmark</a></li><li><a href="#citation" class="table-of-contents__link toc-highlight">Citation</a></li><li><a href="#license" class="table-of-contents__link toc-highlight">License</a></li><li><a href="#acknowledgement" class="table-of-contents__link toc-highlight">Acknowledgement</a></li></ul></div></div></div></main></div><div class="mx-auto flex items-center w-full max-w-[1080px] flex-col px-6 py-12"><div class="flex flex-col gap-6 lg:flex-row lg:items-center lg:justify-between lg:gap-0"><div class="flex items-center gap-4"><div class="flex flex-wrap gap-2 text-sm text-gray-500"><span class="text-inherit">© <!-- -->2024<!-- --> LAMM. Built with Dyte.</span></div></div></div></div></div>
<script src="/assets/js/runtime~main.b10b5ab7.js"></script>
<script src="/assets/js/main.d6ae70d1.js"></script>
</body>
</html>