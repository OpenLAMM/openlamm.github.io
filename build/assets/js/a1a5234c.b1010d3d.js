"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[903],{3905:(t,e,a)=>{a.d(e,{Zo:()=>u,kt:()=>g});var n=a(7294);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function i(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function o(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?i(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function l(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},i=Object.keys(t);for(n=0;n<i.length;n++)a=i[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(t);for(n=0;n<i.length;n++)a=i[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var s=n.createContext({}),d=function(t){var e=n.useContext(s),a=e;return t&&(a="function"==typeof t?t(e):o(o({},e),t)),a},u=function(t){var e=d(t.components);return n.createElement(s.Provider,{value:e},t.children)},c="mdxType",p={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},m=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,i=t.originalType,s=t.parentName,u=l(t,["components","mdxType","originalType","parentName"]),c=d(a),m=r,g=c["".concat(s,".").concat(m)]||c[m]||p[m]||i;return a?n.createElement(g,o(o({ref:e},u),{},{components:a})):n.createElement(g,o({ref:e},u))}));function g(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var i=a.length,o=new Array(i);o[0]=m;var l={};for(var s in e)hasOwnProperty.call(e,s)&&(l[s]=e[s]);l.originalType=t,l[c]="string"==typeof t?t:r,o[1]=l;for(var d=2;d<i;d++)o[d]=a[d];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}m.displayName="MDXCreateElement"},7258:(t,e,a)=>{a.r(e),a.d(e,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>l,toc:()=>d});var n=a(7462),r=(a(7294),a(3905));const i={sidebar_position:3},o="LAMM-Dataset",l={unversionedId:"LAMM-Dataset",id:"LAMM-Dataset",title:"LAMM-Dataset",description:"LAMM-Dataset is a comprehensive multi-modal instruction tuning dataset, which contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets. Here we design four type of multi-modal instruction-response pairs,",source:"@site/docs/LAMM-Dataset.md",sourceDirName:".",slug:"/LAMM-Dataset",permalink:"/docs/LAMM-Dataset",draft:!1,tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Demos",permalink:"/docs/Demos"},next:{title:"LAMM-Framework",permalink:"/docs/LAMM-Framework"}},s={},d=[{value:"Download",id:"download",level:2},{value:"Instruction Data For Training",id:"instruction-data-for-training",level:3},{value:"2D_Instruct data",id:"2d_instruct-data",level:4},{value:"3D_Instruct data",id:"3d_instruct-data",level:4},{value:"Dataset Structure",id:"dataset-structure",level:2},{value:"Meta file format",id:"meta-file-format",level:2},{value:"For images",id:"for-images",level:3},{value:"For point cloud",id:"for-point-cloud",level:3}],u={toc:d},c="wrapper";function p(t){let{components:e,...i}=t;return(0,r.kt)(c,(0,n.Z)({},u,i,{components:e,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"lamm-dataset"},"LAMM-Dataset"),(0,r.kt)("p",null,"LAMM-Dataset is a comprehensive multi-modal instruction tuning dataset, which contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets. Here we design four type of multi-modal instruction-response pairs, "),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"C1: n-round daily dialogue focuses on multi-modal daily conversations. "),(0,r.kt)("li",{parentName:"ul"},"C2: n-round factual knowledge dialogue aims at factual knowledge reasoning. "),(0,r.kt)("li",{parentName:"ul"},"C3: 1-round detailed description aims to elaborate images and 3D scenes in texts. "),(0,r.kt)("li",{parentName:"ul"},"C4: 1-round visual task dialogue transfers various vision tasks into instruction-response pairs, aiming at enhancing generalizability towards domain tasks in other modalities.")),(0,r.kt)("p",null,(0,r.kt)("img",{src:a(1941).Z,width:"1792",height:"1716"})),(0,r.kt)("h2",{id:"download"},"Download"),(0,r.kt)("p",null,"Download LAMM-Dataset from ",(0,r.kt)("a",{parentName:"p",href:"https://opendatalab.com/LAMM/download"},"here"),"."),(0,r.kt)("p",null," If you would like to download the entire LAMM Dataset and LAMM Benchmark, you can do so from the opendatalab website using the provided ",(0,r.kt)("a",{parentName:"p",href:"https://opendatalab.com/LAMM/download"},"LAMM link"),". Here is the table illustrating the correspondence between each Meta file and image collection in the LAMM dataset:"),(0,r.kt)("h3",{id:"instruction-data-for-training"},"Instruction Data For Training"),(0,r.kt)("h4",{id:"2d_instruct-data"},"2D_Instruct data"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,r.kt)("th",{parentName:"tr",align:null},"size"),(0,r.kt)("th",{parentName:"tr",align:null},"Image file name"),(0,r.kt)("th",{parentName:"tr",align:null},"size"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/daily_dialogue_49k.json"},"daily_dialogue_49k.json")),(0,r.kt)("td",{parentName:"tr",align:null},"112M"),(0,r.kt)("td",{parentName:"tr",align:null},"coco_images.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"7.8G")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/detailed_description_49k.json"},"detailed_description_49k.json")),(0,r.kt)("td",{parentName:"tr",align:null},"65.5M"),(0,r.kt)("td",{parentName:"tr",align:null},"coco_images.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"7.8G")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/factual_knowledge_dialogue_42k.json"},"factual_knowledge_dialogue_42k.json")),(0,r.kt)("td",{parentName:"tr",align:null},"83.2M"),(0,r.kt)("td",{parentName:"tr",align:null},"bamboo_images.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"5.4G")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Instruct/meta_file/vision_task_dialogue_46k.json"},"vision_task_dialogue_46k.json")),(0,r.kt)("td",{parentName:"tr",align:null},"64.8M"),(0,r.kt)("td",{parentName:"tr",align:null},"coco_images.zip, bamboo_images.zip, locount_images.zip, textvqa_images.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"9.2G")))),(0,r.kt)("h4",{id:"3d_instruct-data"},"3D_Instruct data"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,r.kt)("th",{parentName:"tr",align:null},"size"),(0,r.kt)("th",{parentName:"tr",align:null},"Image file name"),(0,r.kt)("th",{parentName:"tr",align:null},"size"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/3D_Instruct/meta_file/LAMM_3dinstruct_10k.json"},"LAMM_3dinstruct_10k.json")),(0,r.kt)("td",{parentName:"tr",align:null},"19.6M"),(0,r.kt)("td",{parentName:"tr",align:null},"3rscan_pcls.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"720M")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null}),(0,r.kt)("td",{parentName:"tr",align:null},"shapenet_pcls.zip"),(0,r.kt)("td",{parentName:"tr",align:null},"209M")))),(0,r.kt)("h2",{id:"dataset-structure"},"Dataset Structure"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\u2514\u2500\u2500 2D_Instruct  \n\u2502\xa0\xa0 \u251c\u2500\u2500 coco_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 bamboo_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 textvqa_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 locount_images.zip  \n\u2502\xa0\xa0 \u2514\u2500\u2500 meta_file  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 daily_dialogue_49k.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 detailed_description_49k.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 factual_knowledge_dialogue_42k.json  \n\u2502\xa0\xa0  \xa0\xa0 \u2514\u2500\u2500 vision_task_dialogue_46k.json  \n\u2514\u2500\u2500 3D_Instruct  \n    \u251c\u2500\u2500 3rscan_pcls.zip  \n    \u251c\u2500\u2500 shapenet_pcls.zip  \n    \u2514\u2500\u2500 meta_file  \n     \xa0\xa0 \u2514\u2500\u2500 LAMM_3dinstruct_10k.json  \n")),(0,r.kt)("h2",{id:"meta-file-format"},"Meta file format"),(0,r.kt)("h3",{id:"for-images"},"For images"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'[\n    {\n    "id": "000000019028",  # image id\n    "image": "coco_images/000000019028.jpg", # image path\n    "conversations": [\n        {\n            "from": "human",  # instruction\n            "value": "How is the kitchen in the image furnished?"\n        },\n        {\n            "from": "gpt",  # response\n            "value": "The kitchen in the image is furnished with white cabinets and white appliances. There is a dishwasher, a stove, and a sink. On the stove, a blue towel hangs on the handle. A cutting board is placed on the dishwasher. There are also additional elements like a bowl of apples on the counter and a beige rug on the floor."\n        }\n    ],\n    "task_type": "conversation",  # task type\n    "src_image": "coco2017" # original dataset\n    },\n    {\n        ...\n    }\n]\n')),(0,r.kt)("h3",{id:"for-point-cloud"},"For point cloud"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-json"},'[\n    {\n        "pcl": "shapenet_pcls/04256520_cb71cb7b36dbcb6f826fc8d57346a2e4_4096.npy",\n        "conversations": [\n                {\n                    "from": "human",\n                    "value": "What scenario does this point cloud belong to according to the model\\u2019s prediction?"\n                },\n                {\n                    "from": "gpt",\n                    "value": "Through meticulous analysis, it becomes evident that the point cloud aligns with the characteristics of sofa,couch,lounge s       cenario."\n                }\n            ],\n        "task_type": "classification3d",\n        "src_dataset": "ShapeNet",\n        "src_id": "04256520_cb71cb7b36dbcb6f826fc8d57346a2e4"\n    },\n    {\n        ...\n    }\n]\n')),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Notes"),"\uff1a"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"If you want to work with a specific subset of the LAMM dataset, you will need to download both the corresponding meta file and the image collection. "),(0,r.kt)("li",{parentName:"ol"},"if you prefer to download the data from the official website yourself, you can still organize it in the same way as we have and run it successfully. For example, during the 2D instruction tuning stage, if you only want to run the daily_dialogue_49k.json file, you can download the ",(0,r.kt)("a",{parentName:"li",href:"http://images.cocodataset.org/zips/train2017.zip"},"COCO2017")," dataset and organize it accordingly.")))}p.isMDXComponent=!0},1941:(t,e,a)=>{a.d(e,{Z:()=>n});const n=a.p+"assets/images/LAMM-Dataset-81bee79d9cc35834e52dad859ebd4c04.png"}}]);