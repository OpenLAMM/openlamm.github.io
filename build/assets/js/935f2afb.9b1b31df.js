"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","label":"Overview","href":"/docs/intro","docId":"intro"},{"type":"link","label":"Demos","href":"/docs/Demos","docId":"Demos"},{"type":"link","label":"LAMM-Dataset","href":"/docs/LAMM-Dataset","docId":"LAMM-Dataset"},{"type":"link","label":"LAMM-Framework","href":"/docs/LAMM-Framework","docId":"LAMM-Framework"},{"type":"link","label":"LAMM-Benchmark","href":"/docs/LAMM-Benchmark","docId":"LAMM-Benchmark"},{"type":"link","label":"LAMM Model Zoo","href":"/docs/LAMM-Model-Zoo","docId":"LAMM-Model-Zoo"}]},"docs":{"Demos":{"id":"Demos","title":"Demos","description":"Online Demo","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Overview","description":"Large language models have become a potential pathway toward achieving artificial general intelligence. Recent works on multi-modal large language models have demonstrated their effectiveness in handling visual modalities. In this work, we extend the research of MLLMs to point clouds and present the LAMM-Dataset and LAMM-Benchmark for 2D image and 3D point cloud understanding. We also establish an extensible framework to facilitate the extension of MLLMs to additional modalities.","sidebar":"tutorialSidebar"},"LAMM-Benchmark":{"id":"LAMM-Benchmark","title":"LAMM-Benchmark","description":"LAMM-Benchmark evaluates 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies.","sidebar":"tutorialSidebar"},"LAMM-Dataset":{"id":"LAMM-Dataset","title":"LAMM-Dataset","description":"LAMM-Dataset is a comprehensive multi-modal instruction tuning dataset, which contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.In LAMM-Dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets. Here we design four type of multi-modal instruction-response pairs,","sidebar":"tutorialSidebar"},"LAMM-Framework":{"id":"LAMM-Framework","title":"LAMM-Framework","description":"Installation","sidebar":"tutorialSidebar"},"LAMM-Model-Zoo":{"id":"LAMM-Model-Zoo","title":"LAMM Model Zoo","description":"| # Training Samples  | Vision Encoder | LLM | Training Data | Lora Rank | Link |","sidebar":"tutorialSidebar"}}}')}}]);