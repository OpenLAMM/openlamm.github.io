"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[510],{3905:(t,a,e)=>{e.d(a,{Zo:()=>p,kt:()=>c});var n=e(7294);function l(t,a,e){return a in t?Object.defineProperty(t,a,{value:e,enumerable:!0,configurable:!0,writable:!0}):t[a]=e,t}function r(t,a){var e=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(t,a).enumerable}))),e.push.apply(e,n)}return e}function i(t){for(var a=1;a<arguments.length;a++){var e=null!=arguments[a]?arguments[a]:{};a%2?r(Object(e),!0).forEach((function(a){l(t,a,e[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(e)):r(Object(e)).forEach((function(a){Object.defineProperty(t,a,Object.getOwnPropertyDescriptor(e,a))}))}return t}function m(t,a){if(null==t)return{};var e,n,l=function(t,a){if(null==t)return{};var e,n,l={},r=Object.keys(t);for(n=0;n<r.length;n++)e=r[n],a.indexOf(e)>=0||(l[e]=t[e]);return l}(t,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(t);for(n=0;n<r.length;n++)e=r[n],a.indexOf(e)>=0||Object.prototype.propertyIsEnumerable.call(t,e)&&(l[e]=t[e])}return l}var o=n.createContext({}),s=function(t){var a=n.useContext(o),e=a;return t&&(e="function"==typeof t?t(a):i(i({},a),t)),e},p=function(t){var a=s(t.components);return n.createElement(o.Provider,{value:a},t.children)},d="mdxType",k={inlineCode:"code",wrapper:function(t){var a=t.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(t,a){var e=t.components,l=t.mdxType,r=t.originalType,o=t.parentName,p=m(t,["components","mdxType","originalType","parentName"]),d=s(e),u=l,c=d["".concat(o,".").concat(u)]||d[u]||k[u]||r;return e?n.createElement(c,i(i({ref:a},p),{},{components:e})):n.createElement(c,i({ref:a},p))}));function c(t,a){var e=arguments,l=a&&a.mdxType;if("string"==typeof t||l){var r=e.length,i=new Array(r);i[0]=u;var m={};for(var o in a)hasOwnProperty.call(a,o)&&(m[o]=a[o]);m.originalType=t,m[d]="string"==typeof t?t:l,i[1]=m;for(var s=2;s<r;s++)i[s]=e[s];return n.createElement.apply(null,i)}return n.createElement.apply(null,e)}u.displayName="MDXCreateElement"},2862:(t,a,e)=>{e.r(a),e.d(a,{assets:()=>o,contentTitle:()=>i,default:()=>k,frontMatter:()=>r,metadata:()=>m,toc:()=>s});var n=e(7462),l=(e(7294),e(3905));const r={sidebar_position:5},i="LAMM-Benchmark",m={unversionedId:"LAMM-Benchmark",id:"LAMM-Benchmark",title:"LAMM-Benchmark",description:"LAMM-Benchmark evaluates 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies.",source:"@site/docs/LAMM-Benchmark.md",sourceDirName:".",slug:"/LAMM-Benchmark",permalink:"/docs/LAMM-Benchmark",draft:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"LAMM-Framework",permalink:"/docs/LAMM-Framework"},next:{title:"LAMM Model Zoo",permalink:"/docs/LAMM-Model-Zoo"}},o={},s=[{value:"Data &amp; Model Preparation for LAMM-Benchmark",id:"data--model-preparation-for-lamm-benchmark",level:2},{value:"2D_Benchmark data",id:"2d_benchmark-data",level:3},{value:"3D_Benchmark data",id:"3d_benchmark-data",level:3},{value:"Dataset Structure",id:"dataset-structure",level:3},{value:"Model Preparation",id:"model-preparation",level:3},{value:"Evaluation",id:"evaluation",level:2},{value:"Inference trained models on 2D tasks",id:"inference-trained-models-on-2d-tasks",level:3},{value:"Inference trained models on 3D tasks",id:"inference-trained-models-on-3d-tasks",level:3},{value:"Inference &amp; evaluation trained models on 3D tasks",id:"inference--evaluation-trained-models-on-3d-tasks",level:3},{value:"Evaluation for other MLLM models.",id:"evaluation-for-other-mllm-models",level:3},{value:"GPT Metric",id:"gpt-metric",level:3},{value:"Leaderboard",id:"leaderboard",level:2},{value:"Results of LAMM model on selected 2D vision tasks",id:"results-of-lamm-model-on-selected-2d-vision-tasks",level:3},{value:"Results of 3D tasks by LAMM",id:"results-of-3d-tasks-by-lamm",level:3},{value:"Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs",id:"comparison-of-results-of-binary-locating-metric-and-gpt-metric-of-existing-mllms",level:3},{value:"Comparison of Multimodal Large Language Models on 2D computer vision tasks.",id:"comparison-of-multimodal-large-language-models-on-2d-computer-vision-tasks",level:3}],p={toc:s},d="wrapper";function k(t){let{components:a,...r}=t;return(0,l.kt)(d,(0,n.Z)({},p,r,{components:a,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"lamm-benchmark"},"LAMM-Benchmark"),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"LAMM-Benchmark")," evaluates 9 common image tasks, using a total of 11 datasets with over ",(0,l.kt)("strong",{parentName:"p"},"62,439")," samples, and 3 common point cloud tasks, by utilizing 3 datasets with over ",(0,l.kt)("strong",{parentName:"p"},"12,788")," data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies. "),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"We are the very first attempt to establish a benchmark for MLLMs. We conducted a comprehensive benchmark to quantify the zero-shot and fine-tuning performance of existing multi-modal language models on various computer vision tasks and compare them against state-of-the-art methods of these tasks, including classification, object detection, pose estimation, visual question answering, facial classification, optical character recognition, object counting. "),(0,l.kt)("li",{parentName:"ul"},"We also attempted two novel evaluation strategies designed explicitly for MLLMs. Specifically, as for text generation, we established a scoring logic based on the GPT API. As for tasks involving interactions between points and images, such as object detection and pose estimation, we proposed an object-locating evaluation method.")),(0,l.kt)("p",null,(0,l.kt)("img",{src:e(2094).Z,width:"1608",height:"990"})),(0,l.kt)("h2",{id:"data--model-preparation-for-lamm-benchmark"},"Data & Model Preparation for LAMM-Benchmark"),(0,l.kt)("p",null,"Benchmark Data For Evaluation"),(0,l.kt)("h3",{id:"2d_benchmark-data"},"2D_Benchmark data"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,l.kt)("th",{parentName:"tr",align:null},"size"),(0,l.kt)("th",{parentName:"tr",align:null},"Image file name"),(0,l.kt)("th",{parentName:"tr",align:null},"size"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Caption_flickr30k.json"},"Caption_flickr30k.json")),(0,l.kt)("td",{parentName:"tr",align:null},"598K"),(0,l.kt)("td",{parentName:"tr",align:null},"flickr30k_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"559M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Classification_CIFAR10.json"},"Classification_CIFAR10.json")),(0,l.kt)("td",{parentName:"tr",align:null},"2.6M"),(0,l.kt)("td",{parentName:"tr",align:null},"cifar10_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"8.9M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Counting_FSC147.json"},"Counting_FSC147.json")),(0,l.kt)("td",{parentName:"tr",align:null},"7.3M"),(0,l.kt)("td",{parentName:"tr",align:null},"fsc147_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"44M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Detection_VOC2012.json"},"Detection_VOC2012.json")),(0,l.kt)("td",{parentName:"tr",align:null},"6.4M"),(0,l.kt)("td",{parentName:"tr",align:null},"voc2012_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"196M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Facial_Classification_CelebA(Hair).json"},"Facial_Classification_CelebA(Hair).json")),(0,l.kt)("td",{parentName:"tr",align:null},"2.4M"),(0,l.kt)("td",{parentName:"tr",align:null},"celeba_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"566M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Facial_Classification_CelebA(Smile).json"},"Facial_Classification_CelebA(Smile).json")),(0,l.kt)("td",{parentName:"tr",align:null},"3.7M"),(0,l.kt)("td",{parentName:"tr",align:null},"celeba_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"566M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Fine-grained_Classification_UCMerced.json"},"Fine-grained_Classification_UCMerced.json")),(0,l.kt)("td",{parentName:"tr",align:null},"676K"),(0,l.kt)("td",{parentName:"tr",align:null},"ucmerced_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"317M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Keypoints_Detection_LSP.json"},"Keypoints_Dectection_LSP.json")),(0,l.kt)("td",{parentName:"tr",align:null},"3.9M"),(0,l.kt)("td",{parentName:"tr",align:null},"fsc147_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"44M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Locating_FSC147.json"},"Locating_FSC147.json")),(0,l.kt)("td",{parentName:"tr",align:null},"7.5M"),(0,l.kt)("td",{parentName:"tr",align:null},"fsc147_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"44M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Locating_LSP.json"},"Locating_LSP.json")),(0,l.kt)("td",{parentName:"tr",align:null},"3.9M"),(0,l.kt)("td",{parentName:"tr",align:null},"lsp_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"9.9M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/Locating_VOC2012.json"},"Locating_VOC2012.json")),(0,l.kt)("td",{parentName:"tr",align:null},"6.0M"),(0,l.kt)("td",{parentName:"tr",align:null},"voc2012_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"196M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/OCR_SVT.json"},"OCR_SVT.json")),(0,l.kt)("td",{parentName:"tr",align:null},"68K"),(0,l.kt)("td",{parentName:"tr",align:null},"svt_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"82M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/VQA_AI2D.json"},"VQA_AI2D.json")),(0,l.kt)("td",{parentName:"tr",align:null},"2.1M"),(0,l.kt)("td",{parentName:"tr",align:null},"ai2d_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"559M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/2D_Benchmark/meta_file/VQA_SQAimage.json"},"VQA_SQAimage.json")),(0,l.kt)("td",{parentName:"tr",align:null},"3.6M"),(0,l.kt)("td",{parentName:"tr",align:null},"sqaimage_images.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"127M")))),(0,l.kt)("h3",{id:"3d_benchmark-data"},"3D_Benchmark data"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Meta file name"),(0,l.kt)("th",{parentName:"tr",align:null},"size"),(0,l.kt)("th",{parentName:"tr",align:null},"Image file name"),(0,l.kt)("th",{parentName:"tr",align:null},"size"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/3D_Benchmark/meta_file/Detection_ScanNet.json"},"Detection_ScanNet.json")),(0,l.kt)("td",{parentName:"tr",align:null},"1.7M"),(0,l.kt)("td",{parentName:"tr",align:null},"scannet_pcls.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"246M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/3D_Benchmark/meta_file/VG_ScanRefer.json"},"VG_ScanRefer.json")),(0,l.kt)("td",{parentName:"tr",align:null},"3.7M"),(0,l.kt)("td",{parentName:"tr",align:null},"scannet_pcls.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"246M")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"td",href:"https://huggingface.co/datasets/openlamm/LAMM_Dataset/blob/main/3D_Benchmark/meta_file/VQA_ScanQA_multiplechoice.json"},"VQA_ScanQA_multiplechoice.json")),(0,l.kt)("td",{parentName:"tr",align:null},"859K"),(0,l.kt)("td",{parentName:"tr",align:null},"scannet_pcls.zip"),(0,l.kt)("td",{parentName:"tr",align:null},"246M")))),(0,l.kt)("h3",{id:"dataset-structure"},"Dataset Structure"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"\u251c\u2500\u2500 2D_Benchmark  \n\u2502\xa0\xa0 \u251c\u2500\u2500 ai2d_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 celeba_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 cifar10_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 flickr30k_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 fsc147_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 lsp_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 sqaimage_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 svt_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 ucmerced_images.zip  \n\u2502\xa0\xa0 \u251c\u2500\u2500 voc2012_images.zip  \n\u2502\xa0\xa0 \u2514\u2500\u2500 meta_file  \n\u2502\xa0\xa0   \xa0 \u251c\u2500\u2500 Caption_flickr30k.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Classification_CIFAR10.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Counting_FSC147.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Detection_VOC2012.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Facial_Classification_CelebA(Hair).json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Facial_Classification_CelebA(Smile).json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Fine-grained_Classification_UCMerced.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Keypoints_Dectection_LSP.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Locating_FSC147.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Locating_LSP.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Locating_VOC2012.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 OCR_SVT.json  \n\u2502\xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 VQA_AI2D.json  \n\u2502\xa0\xa0     \u2514\u2500\u2500 VQA_SQAimage.json  \n\u2514\u2500\u2500 3D_Benchmark  \n \xa0\xa0 \u251c\u2500\u2500 scannet_pcls.zip  \n \xa0\xa0 \u2514\u2500\u2500 meta_file  \n \xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 Detection_ScanNet.json  \n \xa0\xa0  \xa0\xa0 \u251c\u2500\u2500 VG_ScanRefer.json  \n \xa0\xa0  \xa0\xa0 \u2514\u2500\u2500 VQA_ScanQA_multiplechoice.json\n")),(0,l.kt)("h3",{id:"model-preparation"},"Model Preparation"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Language Model: Vicuna"),(0,l.kt)("p",{parentName:"li"},"  To prepare the pre-trained Vicuna model, please follow the instructions provided ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/lm-sys/FastChat/tree/main#vicuna-weights"},"Here"),". Put the downloaded model in the ",(0,l.kt)("inlineCode",{parentName:"p"},"./model_zoo/vicuna_ckpt")," folder.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"3D Encoder: EPCL"),(0,l.kt)("p",{parentName:"li"},"  Download Pre-trained EPCL model to tokenize point cloud from ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/openlamm/epcl_vit-L_256tokens/tree/main"},"Here"),". Put the downloaded models in the ",(0,l.kt)("inlineCode",{parentName:"p"},"./model_zoo/epcl_ckpt")," folder.")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"LAMM Models"),(0,l.kt)("p",{parentName:"li"},"  Download LAMM model from ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/OpenLAMM/LAMM/tree/main#lamm-models"},"Here"),". Put the downloaded models in the ",(0,l.kt)("inlineCode",{parentName:"p"},"./ckpt")," folder."),(0,l.kt)("p",{parentName:"li"},"  Or you can train your own LAMM model by following the instructions ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/OpenLAMM/LAMM/tree/main#Training"},"Here"),"!")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("p",{parentName:"li"},"Other Models"),(0,l.kt)("ul",{parentName:"li"},(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://github.com/haotian-liu/LLaVA"},"LLaVA")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://github.com/Vision-CAIR/MiniGPT-4"},"MiniGPT-4")),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("a",{parentName:"li",href:"https://github.com/X-PLUG/mPLUG-Owl"},"mPLUG-owl"))))),(0,l.kt)("h2",{id:"evaluation"},"Evaluation"),(0,l.kt)("h3",{id:"inference-trained-models-on-2d-tasks"},"Inference trained models on 2D tasks"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"cd src\nsh scripts/inference_2D.sh\nor\nsh scripts/inference_2D_slurm.sh       # for slurm\n")),(0,l.kt)("p",null,"Inference & Evaluation on 2D tasks"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/LAMM_2D_Evaluation.sh\n")),(0,l.kt)("p",null,"or "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/LAMM_2D_Evaluation_slurm.sh  # for slurm\n")),(0,l.kt)("h3",{id:"inference-trained-models-on-3d-tasks"},"Inference trained models on 3D tasks"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"cd src\nsh scripts/inference_3D.sh\n")),(0,l.kt)("p",null,"or"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/inference_3D_slurm.sh       # for slurm\n")),(0,l.kt)("h3",{id:"inference--evaluation-trained-models-on-3d-tasks"},"Inference & evaluation trained models on 3D tasks"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/LAMM_3D_Evaluation.sh\n")),(0,l.kt)("p",null,"or "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/LAMM_3D_Evaluation_slurm.sh  # for slurm\n")),(0,l.kt)("h3",{id:"evaluation-for-other-mllm-models"},"Evaluation for other MLLM models."),(0,l.kt)("p",null,"Please refer to ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/haotian-liu/LLaVA"},"LLaVA"),", ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/Vision-CAIR/MiniGPT-4"},"MiniGPT-4")," and ",(0,l.kt)("a",{parentName:"p",href:"https://github.com/X-PLUG/mPLUG-Owl"},"mPLUG-owl")," for inference respectively. Save the answers in ",(0,l.kt)("inlineCode",{parentName:"p"},"./answers"),". And then run ",(0,l.kt)("inlineCode",{parentName:"p"},"common_eval_2d.py")," for evaluation. For example, to evaluate LLaVA on VOC2012:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"python common_eval_2d.py \\\n--dataset-name VOC2012 \\\n--answer-file ./answers/LLaVA \\\n--base-data-path ./data/LAMM-Dataset/2D_Benchmark \\\n2>&1 | tee ./results/LLaVA/eval_VOC2012.log\n")),(0,l.kt)("h3",{id:"gpt-metric"},"GPT Metric"),(0,l.kt)("p",null,"Make sure that you have finished the inference of all the evaluation dataset for both your model/LAMM model and the MLLM model to compare. For example, to rank LAMM and LLaVA: "),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-Bash"},"sh scripts/GPT_metric.sh\n")),(0,l.kt)("p",null,"You may need to dive into scripts to change datasets to evaluation & checkpoints folder to load."),(0,l.kt)("h2",{id:"leaderboard"},"Leaderboard"),(0,l.kt)("h3",{id:"results-of-lamm-model-on-selected-2d-vision-tasks"},"Results of LAMM model on selected 2D vision tasks"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Task"),(0,l.kt)("th",{parentName:"tr",align:null},"Dataset"),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM(Zero-Shot)"),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM(Finetune)"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Classification ",(0,l.kt)("strong",{parentName:"td"},"(Acc)")),(0,l.kt)("td",{parentName:"tr",align:null},"CIFAR10"),(0,l.kt)("td",{parentName:"tr",align:null},"37.90"),(0,l.kt)("td",{parentName:"tr",align:null},"91.2")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Object Detection ",(0,l.kt)("strong",{parentName:"td"},"(Acc)")),(0,l.kt)("td",{parentName:"tr",align:null},"VOC2012"),(0,l.kt)("td",{parentName:"tr",align:null},"7.20"),(0,l.kt)("td",{parentName:"tr",align:null},"13.48")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"VQA ",(0,l.kt)("strong",{parentName:"td"},"(",(0,l.kt)("a",{parentName:"strong",href:"mailto:mAP@0.5"},"mAP@0.5"),")")),(0,l.kt)("td",{parentName:"tr",align:null},"SQAimage"),(0,l.kt)("td",{parentName:"tr",align:null},"49.88"),(0,l.kt)("td",{parentName:"tr",align:null},"74.27")))),(0,l.kt)("h3",{id:"results-of-3d-tasks-by-lamm"},"Results of 3D tasks by LAMM"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Task"),(0,l.kt)("th",{parentName:"tr",align:null},"Dataset"),(0,l.kt)("th",{parentName:"tr",align:null},"SOTA"),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM (Zero-Shot)"),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM (Finetune)"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3D Object Detection ",(0,l.kt)("strong",{parentName:"td"},"(",(0,l.kt)("a",{parentName:"strong",href:"mailto:mAP@0.5"},"mAP@0.5"),")")),(0,l.kt)("td",{parentName:"tr",align:null},"ScanNet"),(0,l.kt)("td",{parentName:"tr",align:null},"63.2"),(0,l.kt)("td",{parentName:"tr",align:null},"8.2"),(0,l.kt)("td",{parentName:"tr",align:null},"11.89")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Visual Grounding ",(0,l.kt)("strong",{parentName:"td"},"(",(0,l.kt)("a",{parentName:"strong",href:"mailto:mAP@0.5"},"mAP@0.5"),")")),(0,l.kt)("td",{parentName:"tr",align:null},"ScanRefer"),(0,l.kt)("td",{parentName:"tr",align:null},"54.59"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"3.38")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"3D VQA ",(0,l.kt)("strong",{parentName:"td"},"(Acc of multiple choice prolblem)")),(0,l.kt)("td",{parentName:"tr",align:null},"ScanQA"),(0,l.kt)("td",{parentName:"tr",align:null},"N/A"),(0,l.kt)("td",{parentName:"tr",align:null},"24.90"),(0,l.kt)("td",{parentName:"tr",align:null},"99.89")))),(0,l.kt)("h3",{id:"comparison-of-results-of-binary-locating-metric-and-gpt-metric-of-existing-mllms"},"Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs"),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null}),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/haotian-liu/LLaVA"},"LLaVA")),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/Vision-CAIR/MiniGPT-4"},"MiniGPT4")),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/X-PLUG/mPLUG-Owl"},"mPLUG-owl")),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Binary-Loc Metric"),(0,l.kt)("td",{parentName:"tr",align:null},"14.73"),(0,l.kt)("td",{parentName:"tr",align:null},"13.12"),(0,l.kt)("td",{parentName:"tr",align:null},"4.42"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"36.53")))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"GPT Metric"),(0,l.kt)("td",{parentName:"tr",align:null},"11"),(0,l.kt)("td",{parentName:"tr",align:null},"-"),(0,l.kt)("td",{parentName:"tr",align:null},"-"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"89")))))),(0,l.kt)("h3",{id:"comparison-of-multimodal-large-language-models-on-2d-computer-vision-tasks"},"Comparison of Multimodal Large Language Models on 2D computer vision tasks."),(0,l.kt)("p",null," Bold fonts for the best results."),(0,l.kt)("table",null,(0,l.kt)("thead",{parentName:"table"},(0,l.kt)("tr",{parentName:"thead"},(0,l.kt)("th",{parentName:"tr",align:null},"Task"),(0,l.kt)("th",{parentName:"tr",align:null},"Dataset"),(0,l.kt)("th",{parentName:"tr",align:null},"Metric"),(0,l.kt)("th",{parentName:"tr",align:null},"SOTA"),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/haotian-liu/LLaVA"},"LLaVA")),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/Vision-CAIR/MiniGPT-4"},"MiniGPT4")),(0,l.kt)("th",{parentName:"tr",align:null},(0,l.kt)("a",{parentName:"th",href:"https://github.com/X-PLUG/mPLUG-Owl"},"mPLUG-owl")),(0,l.kt)("th",{parentName:"tr",align:null},"LAMM"))),(0,l.kt)("tbody",{parentName:"table"},(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Classification"),(0,l.kt)("td",{parentName:"tr",align:null},"CIFAR10"),(0,l.kt)("td",{parentName:"tr",align:null},"Acc \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"99.5"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},"60.83")),(0,l.kt)("td",{parentName:"tr",align:null},"46.22"),(0,l.kt)("td",{parentName:"tr",align:null},"42.5"),(0,l.kt)("td",{parentName:"tr",align:null},"37.9")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Detection"),(0,l.kt)("td",{parentName:"tr",align:null},"VOC2012"),(0,l.kt)("td",{parentName:"tr",align:null},"mAP \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"97.2"),(0,l.kt)("td",{parentName:"tr",align:null},"1.42"),(0,l.kt)("td",{parentName:"tr",align:null},"0.92"),(0,l.kt)("td",{parentName:"tr",align:null},"0.158"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"7.20")))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"VQA"),(0,l.kt)("td",{parentName:"tr",align:null},"SQAimage",(0,l.kt)("br",null),"AI2D"),(0,l.kt)("td",{parentName:"tr",align:null},"Acc \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"92.53",(0,l.kt)("br",null),"N/A"),(0,l.kt)("td",{parentName:"tr",align:null},"40.5",(0,l.kt)("br",null),"18.13"),(0,l.kt)("td",{parentName:"tr",align:null},"43.43",(0,l.kt)("br",null),"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"36.39",(0,l.kt)("br",null),"19.31"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"49.88")),(0,l.kt)("br",null),(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"20.92")))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Image Caption"),(0,l.kt)("td",{parentName:"tr",align:null},"flickr30k"),(0,l.kt)("td",{parentName:"tr",align:null},"BLEU4 \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"30.1"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"6.65"))),(0,l.kt)("td",{parentName:"tr",align:null},"5.1"),(0,l.kt)("td",{parentName:"tr",align:null},"2.74"),(0,l.kt)("td",{parentName:"tr",align:null},"2.56")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"F-g clasification"),(0,l.kt)("td",{parentName:"tr",align:null},"UCMerced"),(0,l.kt)("td",{parentName:"tr",align:null},"Acc \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"100"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"47"))),(0,l.kt)("td",{parentName:"tr",align:null},"33.6"),(0,l.kt)("td",{parentName:"tr",align:null},"32.5"),(0,l.kt)("td",{parentName:"tr",align:null},"18.23")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Counting"),(0,l.kt)("td",{parentName:"tr",align:null},"FSC147"),(0,l.kt)("td",{parentName:"tr",align:null},"MAE \u2193"),(0,l.kt)("td",{parentName:"tr",align:null},"10.79"),(0,l.kt)("td",{parentName:"tr",align:null},"56.2"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"60.67"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"46.88")))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"OCR"),(0,l.kt)("td",{parentName:"tr",align:null},"SVT"),(0,l.kt)("td",{parentName:"tr",align:null},"Word Acc \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"97.9"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"37.78"))),(0,l.kt)("td",{parentName:"tr",align:null},"16.97"),(0,l.kt)("td",{parentName:"tr",align:null},"30.39"),(0,l.kt)("td",{parentName:"tr",align:null},"29.14")),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Facial Classification"),(0,l.kt)("td",{parentName:"tr",align:null},"CelebA(Smile)",(0,l.kt)("br",null),"CelebA(Hair)"),(0,l.kt)("td",{parentName:"tr",align:null},"Acc \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"N/A",(0,l.kt)("br",null),"N/A"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed",(0,l.kt)("br",null),"46.42"),(0,l.kt)("td",{parentName:"tr",align:null},(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"66.36")),(0,l.kt)("br",null),"43.47"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed",(0,l.kt)("br",null),"40.93"),(0,l.kt)("td",{parentName:"tr",align:null},"57.60",(0,l.kt)("br",null)," ",(0,l.kt)("strong",{parentName:"td"},(0,l.kt)("u",null,"56.96")))),(0,l.kt)("tr",{parentName:"tbody"},(0,l.kt)("td",{parentName:"tr",align:null},"Keypoints Detection"),(0,l.kt)("td",{parentName:"tr",align:null},"LSP"),(0,l.kt)("td",{parentName:"tr",align:null},"PCK \u2191"),(0,l.kt)("td",{parentName:"tr",align:null},"99.5"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed"),(0,l.kt)("td",{parentName:"tr",align:null},"Failed")))))}k.isMDXComponent=!0},2094:(t,a,e)=>{e.d(a,{Z:()=>n});const n=e.p+"assets/images/LAMM-benchmark-cc5d25d822c96ad6f81c4dea0fda9502.png"}}]);